{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DRONE LOCALIZATION VALIDATION FRAMEWORK\n",
      "==================================================\n",
      "\n",
      "🎯 VALIDATION METHODS:\n",
      "   🥇 Ground truth validation (using center_pixel coordinates)\n",
      "   • Visual validation (human-interpretable)\n",
      "   • Quantitative metrics (success rates, correlations)\n",
      "   • Synthetic test cases (known ground truth)\n",
      "   • Cross-validation (by altitude/crop type)\n",
      "   • Clustering analysis (spatial coherence)\n",
      "   • Transfer testing (generalization)\n",
      "\n",
      "📊 GROUND TRUTH METRICS:\n",
      "   • Spatial accuracy (distance to true location)\n",
      "   • Top-1/Top-3 localization success\n",
      "   • Real-world error in meters\n",
      "   • Ranking quality analysis\n",
      "\n",
      "📊 TO RUN:\n",
      "   # After training your agent:\n",
      "   validator, results = validate_drone_agent(trainer)\n",
      "\n",
      "🏆 Provides definitive spatial accuracy assessment!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Validation Framework for Drone Localization Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import random\n",
    "\n",
    "class DroneLocalizationValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive validation framework for similarity-based drone localization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trainer, validation_crops: int = 10):\n",
    "        self.trainer = trainer\n",
    "        self.agent = trainer.agent\n",
    "        self.env = trainer.env\n",
    "        self.validation_crops = validation_crops\n",
    "        \n",
    "        # Load ground truth data from JSON\n",
    "        self.ground_truth_data = self._load_ground_truth()\n",
    "        \n",
    "        # Validation results\n",
    "        self.validation_results = {\n",
    "            'ground_truth_validation': {},  # NEW: Using center_pixel coordinates\n",
    "            'visual_validation': [],\n",
    "            'quantitative_metrics': {},\n",
    "            'human_evaluation': [],\n",
    "            'synthetic_tests': {},\n",
    "            'cross_validation': {},\n",
    "            'transfer_test': {}\n",
    "        }\n",
    "        \n",
    "        print(f\"🔍 Validation Framework Initialized\")\n",
    "        print(f\"   Validation crops: {validation_crops}\")\n",
    "        print(f\"   Ground truth coordinates: {len(self.ground_truth_data)} crops\")\n",
    "    \n",
    "    def _load_ground_truth(self) -> Dict:\n",
    "        \"\"\"Load ground truth center_pixel coordinates from JSON\"\"\"\n",
    "        try:\n",
    "            metadata_path = self.env.crops_metadata_path\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            # Create lookup dictionary: filename -> center_pixel\n",
    "            ground_truth = {}\n",
    "            for crop in metadata['crops']:\n",
    "                ground_truth[crop['filename']] = {\n",
    "                    'center_pixel': crop['center_pixel'],\n",
    "                    'crop_size': crop.get('crop_size', 256),\n",
    "                    'crop_index': crop.get('crop_index', 0)\n",
    "                }\n",
    "            \n",
    "            print(f\"   ✅ Loaded ground truth for {len(ground_truth)} crops\")\n",
    "            return ground_truth\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Could not load ground truth: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def pixel_to_grid(self, pixel_x: int, pixel_y: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert pixel coordinates to grid coordinates\"\"\"\n",
    "        tif_h, tif_w = self.env.tif_image.shape[:2]\n",
    "        \n",
    "        grid_x = int((pixel_x / tif_w) * self.env.grid_size)\n",
    "        grid_y = int((pixel_y / tif_h) * self.env.grid_size)\n",
    "        \n",
    "        # Clamp to valid grid range\n",
    "        grid_x = max(0, min(self.env.grid_size - 1, grid_x))\n",
    "        grid_y = max(0, min(self.env.grid_size - 1, grid_y))\n",
    "        \n",
    "        return grid_x, grid_y\n",
    "    \n",
    "    def grid_to_pixel(self, grid_x: int, grid_y: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert grid coordinates to pixel coordinates\"\"\"\n",
    "        tif_h, tif_w = self.env.tif_image.shape[:2]\n",
    "        \n",
    "        pixel_x = int((grid_x / self.env.grid_size) * tif_w)\n",
    "        pixel_y = int((grid_y / self.env.grid_size) * tif_h)\n",
    "        \n",
    "        return pixel_x, pixel_y\n",
    "    \n",
    "    def calculate_spatial_accuracy(self, true_pixel: Tuple[int, int], \n",
    "                                 predicted_grids: List[Tuple[int, int]]) -> Dict:\n",
    "        \"\"\"Calculate spatial accuracy metrics\"\"\"\n",
    "        \n",
    "        # Convert true pixel to grid\n",
    "        true_grid = self.pixel_to_grid(true_pixel[0], true_pixel[1])\n",
    "        \n",
    "        # Calculate distances to all predictions\n",
    "        distances_grid = []\n",
    "        distances_pixel = []\n",
    "        \n",
    "        for pred_grid in predicted_grids:\n",
    "            # Grid distance\n",
    "            grid_dist = np.sqrt((pred_grid[0] - true_grid[0])**2 + (pred_grid[1] - true_grid[1])**2)\n",
    "            distances_grid.append(grid_dist)\n",
    "            \n",
    "            # Convert back to pixel distance for real-world interpretation\n",
    "            pred_pixel = self.grid_to_pixel(pred_grid[0], pred_grid[1])\n",
    "            pixel_dist = np.sqrt((pred_pixel[0] - true_pixel[0])**2 + (pred_pixel[1] - true_pixel[1])**2)\n",
    "            distances_pixel.append(pixel_dist)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        min_distance_grid = min(distances_grid)\n",
    "        min_distance_pixel = min(distances_pixel)\n",
    "        best_rank = distances_grid.index(min_distance_grid)  # 0, 1, or 2\n",
    "        \n",
    "        return {\n",
    "            'true_grid': true_grid,\n",
    "            'predicted_grids': predicted_grids,\n",
    "            'distances_grid': distances_grid,\n",
    "            'distances_pixel': distances_pixel,\n",
    "            'min_distance_grid': min_distance_grid,\n",
    "            'min_distance_pixel': min_distance_pixel,\n",
    "            'best_rank': best_rank,\n",
    "            'top1_accuracy': best_rank == 0,\n",
    "            'top3_accuracy': True,  # Always true since we check top-3\n",
    "            'within_1_grid': min_distance_grid <= 1.0,\n",
    "            'within_2_grid': min_distance_grid <= 2.0,\n",
    "            'within_50_pixels': min_distance_pixel <= 50,\n",
    "            'within_100_pixels': min_distance_pixel <= 100\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_validation(self):\n",
    "        \"\"\"Run all validation tests\"\"\"\n",
    "        \n",
    "        print(f\"\\n🧪\" + \"=\"*60 + \"🧪\")\n",
    "        print(\"    COMPREHENSIVE VALIDATION SUITE\")\n",
    "        print(f\"🧪\" + \"=\"*60 + \"🧪\")\n",
    "        \n",
    "        # 0. Ground Truth Validation (NEW - MOST IMPORTANT!)\n",
    "        print(f\"\\n🎯 0. GROUND TRUTH VALIDATION (Using center_pixel)\")\n",
    "        self.ground_truth_validation()\n",
    "        \n",
    "        # 1. Visual Validation\n",
    "        print(f\"\\n📸 1. VISUAL VALIDATION\")\n",
    "        self.visual_validation()\n",
    "        \n",
    "        # 2. Quantitative Metrics\n",
    "        print(f\"\\n📊 2. QUANTITATIVE METRICS\")\n",
    "        self.quantitative_validation()\n",
    "        \n",
    "        # 3. Synthetic Test Cases\n",
    "        print(f\"\\n🧪 3. SYNTHETIC TEST CASES\")\n",
    "        self.synthetic_validation()\n",
    "        \n",
    "        # 4. Cross-Validation\n",
    "        print(f\"\\n🔄 4. CROSS-VALIDATION\")\n",
    "        self.cross_validation()\n",
    "        \n",
    "        # 5. Clustering Analysis\n",
    "        print(f\"\\n🎯 5. CLUSTERING ANALYSIS\")\n",
    "        self.clustering_validation()\n",
    "        \n",
    "        # 6. Transfer Test (if multiple TIF files available)\n",
    "        print(f\"\\n🚀 6. TRANSFER CAPABILITY TEST\")\n",
    "        self.transfer_validation()\n",
    "        \n",
    "        # 7. Generate validation report\n",
    "        self.generate_validation_report()\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    def ground_truth_validation(self):\n",
    "        \"\"\"Validate using ground truth center_pixel coordinates\"\"\"\n",
    "        \n",
    "        print(f\"   🎯 Testing spatial accuracy using ground truth coordinates...\")\n",
    "        \n",
    "        if not self.ground_truth_data:\n",
    "            print(f\"   ❌ No ground truth data available\")\n",
    "            return\n",
    "        \n",
    "        # Test all available crops with ground truth\n",
    "        all_results = []\n",
    "        test_crops = list(self.ground_truth_data.keys())[:self.validation_crops]\n",
    "        \n",
    "        for crop_filename in test_crops:\n",
    "            try:\n",
    "                # Load specific crop\n",
    "                crop_path = Path(\"realistic_drone_crops\") / crop_filename\n",
    "                if not crop_path.exists():\n",
    "                    continue\n",
    "                \n",
    "                crop_image = cv2.imread(str(crop_path))\n",
    "                crop_image = cv2.cvtColor(crop_image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Set current crop in environment\n",
    "                self.env.current_crop = crop_image\n",
    "                self.env.current_metadata = {'filename': crop_filename}\n",
    "                \n",
    "                # Get agent predictions\n",
    "                locations, probabilities, _ = self.agent.select_top3_actions(self.env.tif_image, crop_image)\n",
    "                \n",
    "                # Get ground truth\n",
    "                ground_truth = self.ground_truth_data[crop_filename]\n",
    "                true_pixel = tuple(ground_truth['center_pixel'])\n",
    "                \n",
    "                # Calculate spatial accuracy\n",
    "                spatial_metrics = self.calculate_spatial_accuracy(true_pixel, locations)\n",
    "                \n",
    "                # Also calculate similarity for comparison\n",
    "                reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "                \n",
    "                result = {\n",
    "                    'crop_filename': crop_filename,\n",
    "                    'true_pixel': true_pixel,\n",
    "                    'predicted_locations': locations,\n",
    "                    'probabilities': probabilities,\n",
    "                    'similarities': similarities,\n",
    "                    'spatial_metrics': spatial_metrics,\n",
    "                    'reward': reward\n",
    "                }\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error testing {crop_filename}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_results:\n",
    "            print(f\"   ❌ No successful ground truth tests\")\n",
    "            return\n",
    "        \n",
    "        # Aggregate results\n",
    "        spatial_accuracy_metrics = {\n",
    "            'total_tested': len(all_results),\n",
    "            'top1_accuracy': np.mean([r['spatial_metrics']['top1_accuracy'] for r in all_results]),\n",
    "            'within_1_grid': np.mean([r['spatial_metrics']['within_1_grid'] for r in all_results]),\n",
    "            'within_2_grid': np.mean([r['spatial_metrics']['within_2_grid'] for r in all_results]),\n",
    "            'within_50_pixels': np.mean([r['spatial_metrics']['within_50_pixels'] for r in all_results]),\n",
    "            'within_100_pixels': np.mean([r['spatial_metrics']['within_100_pixels'] for r in all_results]),\n",
    "            'mean_distance_grid': np.mean([r['spatial_metrics']['min_distance_grid'] for r in all_results]),\n",
    "            'mean_distance_pixel': np.mean([r['spatial_metrics']['min_distance_pixel'] for r in all_results]),\n",
    "            'median_distance_pixel': np.median([r['spatial_metrics']['min_distance_pixel'] for r in all_results]),\n",
    "            'rank_distribution': {\n",
    "                'rank_0': np.mean([r['spatial_metrics']['best_rank'] == 0 for r in all_results]),\n",
    "                'rank_1': np.mean([r['spatial_metrics']['best_rank'] == 1 for r in all_results]),\n",
    "                'rank_2': np.mean([r['spatial_metrics']['best_rank'] == 2 for r in all_results])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.validation_results['ground_truth_validation'] = {\n",
    "            'metrics': spatial_accuracy_metrics,\n",
    "            'detailed_results': all_results\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"   📈 Ground Truth Validation Results:\")\n",
    "        print(f\"      Crops tested: {spatial_accuracy_metrics['total_tested']}\")\n",
    "        print(f\"      Top-1 spatial accuracy: {spatial_accuracy_metrics['top1_accuracy']:.1%}\")\n",
    "        print(f\"      Within 1 grid cell: {spatial_accuracy_metrics['within_1_grid']:.1%}\")\n",
    "        print(f\"      Within 2 grid cells: {spatial_accuracy_metrics['within_2_grid']:.1%}\")\n",
    "        print(f\"      Within 50 pixels: {spatial_accuracy_metrics['within_50_pixels']:.1%}\")\n",
    "        print(f\"      Within 100 pixels: {spatial_accuracy_metrics['within_100_pixels']:.1%}\")\n",
    "        print(f\"      Mean distance: {spatial_accuracy_metrics['mean_distance_pixel']:.1f} pixels\")\n",
    "        print(f\"      Median distance: {spatial_accuracy_metrics['median_distance_pixel']:.1f} pixels\")\n",
    "        \n",
    "        # Plot spatial accuracy visualization\n",
    "        self._plot_spatial_accuracy(all_results)\n",
    "    \n",
    "    def _plot_spatial_accuracy(self, results: List[Dict]):\n",
    "        \"\"\"Plot spatial accuracy visualization\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Distance distribution\n",
    "        distances = [r['spatial_metrics']['min_distance_pixel'] for r in results]\n",
    "        axes[0,0].hist(distances, bins=15, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[0,0].axvline(x=50, color='red', linestyle='--', label='50px threshold')\n",
    "        axes[0,0].axvline(x=100, color='orange', linestyle='--', label='100px threshold')\n",
    "        axes[0,0].set_title('Distance to True Location (pixels)')\n",
    "        axes[0,0].set_xlabel('Distance (pixels)')\n",
    "        axes[0,0].set_ylabel('Count')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # 2. Rank distribution\n",
    "        ranks = [r['spatial_metrics']['best_rank'] for r in results]\n",
    "        rank_counts = [ranks.count(i) for i in range(3)]\n",
    "        axes[0,1].bar(['Top-1', 'Top-2', 'Top-3'], rank_counts, color=['gold', 'silver', 'bronze'])\n",
    "        axes[0,1].set_title('Best Prediction Rank Distribution')\n",
    "        axes[0,1].set_ylabel('Count')\n",
    "        \n",
    "        # 3. Spatial scatter plot of errors\n",
    "        true_positions = np.array([r['true_pixel'] for r in results])\n",
    "        predicted_positions = []\n",
    "        \n",
    "        for r in results:\n",
    "            best_rank = r['spatial_metrics']['best_rank']\n",
    "            best_grid = r['predicted_locations'][best_rank]\n",
    "            best_pixel = self.grid_to_pixel(best_grid[0], best_grid[1])\n",
    "            predicted_positions.append(best_pixel)\n",
    "        \n",
    "        predicted_positions = np.array(predicted_positions)\n",
    "        \n",
    "        axes[0,2].scatter(true_positions[:, 0], true_positions[:, 1], \n",
    "                         c='red', label='True positions', s=50, alpha=0.7)\n",
    "        axes[0,2].scatter(predicted_positions[:, 0], predicted_positions[:, 1], \n",
    "                         c='blue', label='Best predictions', s=50, alpha=0.7)\n",
    "        \n",
    "        # Draw lines connecting true to predicted\n",
    "        for i in range(len(true_positions)):\n",
    "            axes[0,2].plot([true_positions[i,0], predicted_positions[i,0]], \n",
    "                          [true_positions[i,1], predicted_positions[i,1]], \n",
    "                          'gray', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        axes[0,2].set_title('True vs Predicted Positions')\n",
    "        axes[0,2].set_xlabel('X (pixels)')\n",
    "        axes[0,2].set_ylabel('Y (pixels)')\n",
    "        axes[0,2].legend()\n",
    "        axes[0,2].set_aspect('equal')\n",
    "        \n",
    "        # 4. Distance vs similarity correlation\n",
    "        similarities = [max(r['similarities']) for r in results]\n",
    "        axes[1,0].scatter(distances, similarities, alpha=0.7)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(distances, similarities)[0,1]\n",
    "        axes[1,0].set_title(f'Distance vs Similarity\\n(Correlation: {correlation:.3f})')\n",
    "        axes[1,0].set_xlabel('Distance to true location (pixels)')\n",
    "        axes[1,0].set_ylabel('Best similarity score')\n",
    "        \n",
    "        # 5. Accuracy by threshold\n",
    "        thresholds = [25, 50, 75, 100, 150, 200]\n",
    "        accuracies = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            accuracy = np.mean([d <= threshold for d in distances])\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        axes[1,1].plot(thresholds, accuracies, 'o-', linewidth=2, markersize=8)\n",
    "        axes[1,1].set_title('Accuracy vs Distance Threshold')\n",
    "        axes[1,1].set_xlabel('Distance threshold (pixels)')\n",
    "        axes[1,1].set_ylabel('Accuracy')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Individual crop performance\n",
    "        crop_names = [r['crop_filename'][:20] + '...' if len(r['crop_filename']) > 20 \n",
    "                     else r['crop_filename'] for r in results[:10]]  # Show first 10\n",
    "        crop_distances = distances[:10]\n",
    "        \n",
    "        colors = ['green' if d <= 50 else 'orange' if d <= 100 else 'red' for d in crop_distances]\n",
    "        \n",
    "        bars = axes[1,2].bar(range(len(crop_names)), crop_distances, color=colors, alpha=0.7)\n",
    "        axes[1,2].set_title('Distance by Crop (First 10)')\n",
    "        axes[1,2].set_xlabel('Crop')\n",
    "        axes[1,2].set_ylabel('Distance (pixels)')\n",
    "        axes[1,2].set_xticks(range(len(crop_names)))\n",
    "        axes[1,2].set_xticklabels(crop_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add threshold lines\n",
    "        axes[1,2].axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50px')\n",
    "        axes[1,2].axhline(y=100, color='orange', linestyle='--', alpha=0.7, label='100px')\n",
    "        axes[1,2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Ground Truth Spatial Accuracy Analysis', fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def visual_validation(self):\n",
    "        \"\"\"Visual validation - show predictions for human evaluation\"\"\"\n",
    "        \n",
    "        print(f\"   📸 Generating visual validation examples...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "        \n",
    "        for i in range(3):  # Test 3 different crops\n",
    "            # Reset environment\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            \n",
    "            # Get agent predictions\n",
    "            locations, probabilities, _ = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "            \n",
    "            # Calculate similarities for validation\n",
    "            reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "            \n",
    "            # Show original crop\n",
    "            axes[i, 0].imshow(crop_image)\n",
    "            axes[i, 0].set_title(f'Original Crop\\n{state[\"metadata\"][\"filename\"]}')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Show top 3 predictions\n",
    "            for j, (loc, prob, sim) in enumerate(zip(locations, probabilities, similarities)):\n",
    "                grid_x, grid_y = loc\n",
    "                predicted_area = self.env.extract_area_at_grid(grid_x, grid_y)\n",
    "                \n",
    "                axes[i, j+1].imshow(predicted_area)\n",
    "                axes[i, j+1].set_title(f'Pred #{j+1}: Grid({grid_x},{grid_y})\\n'\n",
    "                                     f'Confidence: {prob:.2%}\\n'\n",
    "                                     f'Similarity: {sim:.3f}')\n",
    "                axes[i, j+1].axis('off')\n",
    "                \n",
    "                # Color-code border by quality\n",
    "                if sim > 0.7:\n",
    "                    border_color = 'green'\n",
    "                elif sim > 0.5:\n",
    "                    border_color = 'orange' \n",
    "                else:\n",
    "                    border_color = 'red'\n",
    "                \n",
    "                for spine in axes[i, j+1].spines.values():\n",
    "                    spine.set_color(border_color)\n",
    "                    spine.set_linewidth(3)\n",
    "            \n",
    "            # Store for analysis\n",
    "            self.validation_results['visual_validation'].append({\n",
    "                'crop_file': state[\"metadata\"][\"filename\"],\n",
    "                'predictions': locations,\n",
    "                'probabilities': probabilities,\n",
    "                'similarities': similarities,\n",
    "                'reward': reward\n",
    "            })\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Visual Validation - Agent Predictions\\n🟢 Good (>0.7) 🟠 OK (>0.5) 🔴 Poor (<0.5)', \n",
    "                     fontsize=16, y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Analysis\n",
    "        all_similarities = [item['similarities'] for item in self.validation_results['visual_validation']]\n",
    "        flat_similarities = [sim for sublist in all_similarities for sim in sublist]\n",
    "        \n",
    "        print(f\"   📊 Visual Validation Results:\")\n",
    "        print(f\"      Average similarity: {np.mean(flat_similarities):.3f}\")\n",
    "        print(f\"      Best similarity: {np.max(flat_similarities):.3f}\")\n",
    "        print(f\"      Good predictions (>0.7): {sum(1 for s in flat_similarities if s > 0.7)}/{len(flat_similarities)}\")\n",
    "    \n",
    "    def quantitative_validation(self):\n",
    "        \"\"\"Quantitative metrics validation\"\"\"\n",
    "        \n",
    "        print(f\"   📊 Computing quantitative metrics...\")\n",
    "        \n",
    "        # Test on multiple crops\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(self.validation_crops):\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            locations, probabilities, _ = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "            reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "            \n",
    "            result = {\n",
    "                'crop_id': i,\n",
    "                'similarities': similarities,\n",
    "                'probabilities': probabilities,\n",
    "                'reward': reward,\n",
    "                'top1_similarity': similarities[0],\n",
    "                'max_similarity': max(similarities),\n",
    "                'confidence_accuracy_correlation': np.corrcoef(probabilities, similarities)[0,1]\n",
    "            }\n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        metrics = {\n",
    "            'mean_top1_similarity': np.mean([r['top1_similarity'] for r in all_results]),\n",
    "            'mean_max_similarity': np.mean([r['max_similarity'] for r in all_results]),\n",
    "            'mean_reward': np.mean([r['reward'] for r in all_results]),\n",
    "            'std_reward': np.std([r['reward'] for r in all_results]),\n",
    "            'top1_threshold_70': sum(1 for r in all_results if r['top1_similarity'] > 0.7) / len(all_results),\n",
    "            'top3_threshold_70': sum(1 for r in all_results if r['max_similarity'] > 0.7) / len(all_results),\n",
    "            'confidence_correlation': np.mean([r['confidence_accuracy_correlation'] for r in all_results if not np.isnan(r['confidence_accuracy_correlation'])])\n",
    "        }\n",
    "        \n",
    "        self.validation_results['quantitative_metrics'] = metrics\n",
    "        \n",
    "        print(f\"   📈 Quantitative Results:\")\n",
    "        print(f\"      Mean Top-1 Similarity: {metrics['mean_top1_similarity']:.3f}\")\n",
    "        print(f\"      Mean Max Similarity: {metrics['mean_max_similarity']:.3f}\")\n",
    "        print(f\"      Top-1 Success Rate (>0.7): {metrics['top1_threshold_70']:.1%}\")\n",
    "        print(f\"      Top-3 Success Rate (>0.7): {metrics['top3_threshold_70']:.1%}\")\n",
    "        print(f\"      Confidence-Accuracy Correlation: {metrics['confidence_correlation']:.3f}\")\n",
    "        \n",
    "        # Plot metrics distribution\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Similarity distribution\n",
    "        all_sims = [sim for r in all_results for sim in r['similarities']]\n",
    "        axes[0,0].hist(all_sims, bins=20, alpha=0.7, color='blue')\n",
    "        axes[0,0].axvline(x=0.7, color='red', linestyle='--', label='Good threshold')\n",
    "        axes[0,0].set_title('Similarity Score Distribution')\n",
    "        axes[0,0].set_xlabel('Similarity Score')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Reward distribution\n",
    "        rewards = [r['reward'] for r in all_results]\n",
    "        axes[0,1].hist(rewards, bins=20, alpha=0.7, color='green')\n",
    "        axes[0,1].set_title('Reward Distribution')\n",
    "        axes[0,1].set_xlabel('Reward')\n",
    "        \n",
    "        # Confidence vs Similarity scatter\n",
    "        all_probs = [prob for r in all_results for prob in r['probabilities']]\n",
    "        axes[1,0].scatter(all_probs, all_sims, alpha=0.6)\n",
    "        axes[1,0].set_xlabel('Confidence')\n",
    "        axes[1,0].set_ylabel('Similarity')\n",
    "        axes[1,0].set_title('Confidence vs Similarity')\n",
    "        \n",
    "        # Success rate by ranking\n",
    "        rank_success = []\n",
    "        for rank in range(3):\n",
    "            rank_sims = [r['similarities'][rank] for r in all_results]\n",
    "            success_rate = sum(1 for s in rank_sims if s > 0.7) / len(rank_sims)\n",
    "            rank_success.append(success_rate)\n",
    "        \n",
    "        axes[1,1].bar(['Top-1', 'Top-2', 'Top-3'], rank_success, color=['gold', 'silver', 'bronze'])\n",
    "        axes[1,1].set_title('Success Rate by Ranking')\n",
    "        axes[1,1].set_ylabel('Success Rate (>0.7 similarity)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def synthetic_validation(self):\n",
    "        \"\"\"Test with synthetic/known cases\"\"\"\n",
    "        \n",
    "        print(f\"   🧪 Creating synthetic test cases...\")\n",
    "        \n",
    "        # Test 1: Identity test (crop vs itself)\n",
    "        identity_results = []\n",
    "        \n",
    "        for i in range(5):\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            \n",
    "            # Create a synthetic case: place the crop in a known location\n",
    "            h, w = tif_image.shape[:2]\n",
    "            \n",
    "            # Place crop in center of TIF\n",
    "            center_x, center_y = w // 2, h // 2\n",
    "            crop_h, crop_w = crop_image.shape[:2]\n",
    "            \n",
    "            # Insert crop into TIF at center\n",
    "            synthetic_tif = tif_image.copy()\n",
    "            x1 = center_x - crop_w // 2\n",
    "            y1 = center_y - crop_h // 2\n",
    "            x2 = x1 + crop_w\n",
    "            y2 = y1 + crop_h\n",
    "            \n",
    "            if x1 >= 0 and y1 >= 0 and x2 < w and y2 < h:\n",
    "                synthetic_tif[y1:y2, x1:x2] = crop_image\n",
    "                \n",
    "                # Test agent on this synthetic case\n",
    "                locations, probabilities, _ = self.agent.select_top3_actions(synthetic_tif, crop_image)\n",
    "                \n",
    "                # Check if agent finds the center location\n",
    "                center_grid_x = int((center_x / w) * self.env.grid_size)\n",
    "                center_grid_y = int((center_y / h) * self.env.grid_size)\n",
    "                \n",
    "                # Find closest prediction to center\n",
    "                distances = []\n",
    "                for loc in locations:\n",
    "                    dist = np.sqrt((loc[0] - center_grid_x)**2 + (loc[1] - center_grid_y)**2)\n",
    "                    distances.append(dist)\n",
    "                \n",
    "                min_distance = min(distances)\n",
    "                best_rank = distances.index(min_distance)\n",
    "                \n",
    "                identity_results.append({\n",
    "                    'min_distance': min_distance,\n",
    "                    'best_rank': best_rank,\n",
    "                    'found_exact': min_distance <= 1.0  # Within 1 grid cell\n",
    "                })\n",
    "        \n",
    "        synthetic_metrics = {\n",
    "            'identity_mean_distance': np.mean([r['min_distance'] for r in identity_results]),\n",
    "            'identity_success_rate': np.mean([r['found_exact'] for r in identity_results]),\n",
    "            'identity_top1_rate': np.mean([r['best_rank'] == 0 for r in identity_results])\n",
    "        }\n",
    "        \n",
    "        self.validation_results['synthetic_tests'] = synthetic_metrics\n",
    "        \n",
    "        print(f\"   🎯 Synthetic Test Results:\")\n",
    "        print(f\"      Identity test success rate: {synthetic_metrics['identity_success_rate']:.1%}\")\n",
    "        print(f\"      Mean distance to planted crop: {synthetic_metrics['identity_mean_distance']:.2f} grid cells\")\n",
    "        print(f\"      Top-1 detection rate: {synthetic_metrics['identity_top1_rate']:.1%}\")\n",
    "    \n",
    "    def cross_validation(self):\n",
    "        \"\"\"Cross-validation across different crop types/altitudes\"\"\"\n",
    "        \n",
    "        print(f\"   🔄 Cross-validation by crop characteristics...\")\n",
    "        \n",
    "        # Group crops by altitude if available\n",
    "        altitude_groups = {}\n",
    "        other_crops = []\n",
    "        \n",
    "        for crop_data in self.env.crops_data:\n",
    "            altitude = crop_data.get('altitude_meters', None)\n",
    "            if altitude:\n",
    "                if altitude not in altitude_groups:\n",
    "                    altitude_groups[altitude] = []\n",
    "                altitude_groups[altitude].append(crop_data)\n",
    "            else:\n",
    "                other_crops.append(crop_data)\n",
    "        \n",
    "        print(f\"      Found {len(altitude_groups)} altitude groups: {list(altitude_groups.keys())}\")\n",
    "        \n",
    "        # Test performance by altitude\n",
    "        altitude_performance = {}\n",
    "        \n",
    "        for altitude, crops in altitude_groups.items():\n",
    "            if len(crops) >= 3:  # Need at least 3 crops for meaningful test\n",
    "                similarities = []\n",
    "                \n",
    "                for crop_data in crops[:5]:  # Test up to 5 crops per altitude\n",
    "                    # Manually set the crop for testing\n",
    "                    self.env.current_metadata = crop_data\n",
    "                    crop_path = Path(\"realistic_drone_crops\") / crop_data['filename']\n",
    "                    crop_image = cv2.imread(str(crop_path))\n",
    "                    self.env.current_crop = cv2.cvtColor(crop_image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    locations, probabilities, _ = self.agent.select_top3_actions(self.env.tif_image, self.env.current_crop)\n",
    "                    reward, sims = self.env.calculate_reward(locations, probabilities)\n",
    "                    \n",
    "                    similarities.extend(sims)\n",
    "                \n",
    "                altitude_performance[altitude] = {\n",
    "                    'mean_similarity': np.mean(similarities),\n",
    "                    'max_similarity': np.max(similarities),\n",
    "                    'success_rate': sum(1 for s in similarities if s > 0.7) / len(similarities)\n",
    "                }\n",
    "        \n",
    "        self.validation_results['cross_validation'] = altitude_performance\n",
    "        \n",
    "        print(f\"   📊 Cross-validation Results:\")\n",
    "        for altitude, perf in altitude_performance.items():\n",
    "            print(f\"      {altitude}m altitude: \"\n",
    "                  f\"Mean sim: {perf['mean_similarity']:.3f}, \"\n",
    "                  f\"Success: {perf['success_rate']:.1%}\")\n",
    "    \n",
    "    def clustering_validation(self):\n",
    "        \"\"\"Validate that agent groups similar terrains together\"\"\"\n",
    "        \n",
    "        print(f\"   🎯 Analyzing spatial clustering of predictions...\")\n",
    "        \n",
    "        # Collect predictions for multiple crops\n",
    "        all_predictions = []\n",
    "        crop_types = []\n",
    "        \n",
    "        for i in range(min(10, len(self.env.crops_data))):\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            locations, probabilities, _ = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "            \n",
    "            # Store top prediction location\n",
    "            all_predictions.append(locations[0])\n",
    "            crop_types.append(state['metadata'].get('altitude_meters', 'unknown'))\n",
    "        \n",
    "        # Analyze spatial distribution\n",
    "        if len(all_predictions) > 3:\n",
    "            coords = np.array(all_predictions)\n",
    "            \n",
    "            # Calculate clustering tendency\n",
    "            from sklearn.cluster import KMeans\n",
    "            \n",
    "            # Try different numbers of clusters\n",
    "            inertias = []\n",
    "            k_range = range(2, min(6, len(all_predictions)))\n",
    "            \n",
    "            for k in k_range:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                kmeans.fit(coords)\n",
    "                inertias.append(kmeans.inertia_)\n",
    "            \n",
    "            # Find elbow point (optimal clusters)\n",
    "            if len(inertias) > 1:\n",
    "                diffs = np.diff(inertias)\n",
    "                optimal_k = np.argmin(diffs) + 2  # +2 because range starts at 2\n",
    "                \n",
    "                print(f\"      Spatial analysis: {len(all_predictions)} predictions\")\n",
    "                print(f\"      Optimal clusters: {optimal_k}\")\n",
    "                print(f\"      Spread: {np.std(coords, axis=0)} (grid units)\")\n",
    "    \n",
    "    def transfer_validation(self):\n",
    "        \"\"\"Test transfer capability (placeholder for multiple TIF files)\"\"\"\n",
    "        \n",
    "        print(f\"   🚀 Transfer capability test...\")\n",
    "        print(f\"      Note: Requires multiple TIF files for full transfer test\")\n",
    "        print(f\"      Current test: Robustness across different regions of same TIF\")\n",
    "        \n",
    "        # Test on different regions of the same TIF\n",
    "        regions_tested = 0\n",
    "        region_performances = []\n",
    "        \n",
    "        for region in ['top-left', 'top-right', 'bottom-left', 'bottom-right', 'center']:\n",
    "            # Modify the environment to focus on different TIF regions\n",
    "            # This is a simplified transfer test\n",
    "            performance = self._test_tif_region(region)\n",
    "            if performance:\n",
    "                region_performances.append(performance)\n",
    "                regions_tested += 1\n",
    "        \n",
    "        if region_performances:\n",
    "            transfer_metrics = {\n",
    "                'regions_tested': regions_tested,\n",
    "                'mean_region_performance': np.mean(region_performances),\n",
    "                'region_consistency': 1.0 - np.std(region_performances)  # Higher = more consistent\n",
    "            }\n",
    "            \n",
    "            self.validation_results['transfer_test'] = transfer_metrics\n",
    "            \n",
    "            print(f\"      Regions tested: {regions_tested}\")\n",
    "            print(f\"      Mean performance: {transfer_metrics['mean_region_performance']:.3f}\")\n",
    "            print(f\"      Consistency: {transfer_metrics['region_consistency']:.3f}\")\n",
    "    \n",
    "    def _test_tif_region(self, region: str) -> Optional[float]:\n",
    "        \"\"\"Test performance on specific TIF region\"\"\"\n",
    "        # Simplified implementation - would be expanded for full transfer testing\n",
    "        try:\n",
    "            # Test a few crops and return average similarity\n",
    "            similarities = []\n",
    "            for _ in range(3):\n",
    "                tif_image, crop_image, state = self.env.reset()\n",
    "                locations, probabilities, _ = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "                reward, sims = self.env.calculate_reward(locations, probabilities)\n",
    "                similarities.extend(sims)\n",
    "            \n",
    "            return np.mean(similarities)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def generate_validation_report(self):\n",
    "        \"\"\"Generate comprehensive validation report\"\"\"\n",
    "        \n",
    "        print(f\"\\n📋\" + \"=\"*60 + \"📋\")\n",
    "        print(\"    VALIDATION REPORT\")\n",
    "        print(f\"📋\" + \"=\"*60 + \"📋\")\n",
    "        \n",
    "        # Primary metrics from ground truth validation\n",
    "        if 'ground_truth_validation' in self.validation_results:\n",
    "            gt_metrics = self.validation_results['ground_truth_validation']['metrics']\n",
    "            \n",
    "            print(f\"\\n🎯 SPATIAL ACCURACY (GROUND TRUTH):\")\n",
    "            print(f\"   Crops tested: {gt_metrics['total_tested']}\")\n",
    "            print(f\"   Top-1 Spatial Accuracy: {gt_metrics['top1_accuracy']:.1%}\")\n",
    "            print(f\"   Within 1 grid cell (±{self.env.grid_size} pixels): {gt_metrics['within_1_grid']:.1%}\")\n",
    "            print(f\"   Within 2 grid cells: {gt_metrics['within_2_grid']:.1%}\")\n",
    "            print(f\"   Within 50 pixels: {gt_metrics['within_50_pixels']:.1%}\")\n",
    "            print(f\"   Within 100 pixels: {gt_metrics['within_100_pixels']:.1%}\")\n",
    "            print(f\"   Mean distance error: {gt_metrics['mean_distance_pixel']:.1f} pixels\")\n",
    "            print(f\"   Median distance error: {gt_metrics['median_distance_pixel']:.1f} pixels\")\n",
    "            \n",
    "            # Primary performance grade based on spatial accuracy\n",
    "            spatial_accuracy_50px = gt_metrics['within_50_pixels']\n",
    "            \n",
    "            if spatial_accuracy_50px >= 0.8:\n",
    "                grade = \"🏆 EXCELLENT\"\n",
    "                interpretation = \"Agent accurately localizes crops\"\n",
    "            elif spatial_accuracy_50px >= 0.6:\n",
    "                grade = \"🥇 GOOD\"\n",
    "                interpretation = \"Agent shows good spatial understanding\"\n",
    "            elif spatial_accuracy_50px >= 0.4:\n",
    "                grade = \"🥈 FAIR\"\n",
    "                interpretation = \"Agent has basic localization ability\"\n",
    "            else:\n",
    "                grade = \"🥉 NEEDS IMPROVEMENT\"\n",
    "                interpretation = \"Agent needs more training\"\n",
    "            \n",
    "            print(f\"\\n📊 SPATIAL ACCURACY GRADE: {grade}\")\n",
    "            print(f\"   {interpretation}\")\n",
    "        \n",
    "        # Secondary metrics from similarity validation\n",
    "        if 'quantitative_metrics' in self.validation_results:\n",
    "            sim_metrics = self.validation_results['quantitative_metrics']\n",
    "            \n",
    "            print(f\"\\n🎨 SIMILARITY PERFORMANCE:\")\n",
    "            print(f\"   Mean Similarity Score: {sim_metrics['mean_top1_similarity']:.3f}\")\n",
    "            print(f\"   Similarity Success Rate (>0.7): {sim_metrics['top1_threshold_70']:.1%}\")\n",
    "            print(f\"   Confidence Calibration: {sim_metrics['confidence_correlation']:.3f}\")\n",
    "        \n",
    "        # Performance insights\n",
    "        print(f\"\\n💡 VALIDATION INSIGHTS:\")\n",
    "        \n",
    "        if 'ground_truth_validation' in self.validation_results:\n",
    "            gt_metrics = self.validation_results['ground_truth_validation']['metrics']\n",
    "            \n",
    "            # Spatial accuracy insights\n",
    "            mean_distance = gt_metrics['mean_distance_pixel']\n",
    "            median_distance = gt_metrics['median_distance_pixel']\n",
    "            \n",
    "            if mean_distance <= 50:\n",
    "                print(f\"   ✅ Excellent spatial precision (mean error: {mean_distance:.1f}px)\")\n",
    "            elif mean_distance <= 100:\n",
    "                print(f\"   ⚠️ Good spatial precision (mean error: {mean_distance:.1f}px)\")\n",
    "            else:\n",
    "                print(f\"   ❌ Poor spatial precision (mean error: {mean_distance:.1f}px)\")\n",
    "            \n",
    "            # Consistency check\n",
    "            if abs(mean_distance - median_distance) < 20:\n",
    "                print(f\"   ✅ Consistent performance across crops\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Inconsistent performance (some crops much harder)\")\n",
    "            \n",
    "            # Ranking analysis\n",
    "            rank_dist = gt_metrics['rank_distribution']\n",
    "            if rank_dist['rank_0'] > 0.6:\n",
    "                print(f\"   ✅ Agent confidently picks best locations first\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Agent ranking could be improved\")\n",
    "        \n",
    "        # Cross-validation insights\n",
    "        if 'cross_validation' in self.validation_results:\n",
    "            cv_results = self.validation_results['cross_validation']\n",
    "            if cv_results:\n",
    "                print(f\"   📊 Performance varies by altitude/crop type\")\n",
    "                best_altitude = max(cv_results.keys(), key=lambda k: cv_results[k]['mean_similarity'])\n",
    "                worst_altitude = min(cv_results.keys(), key=lambda k: cv_results[k]['mean_similarity'])\n",
    "                print(f\"      Best: {best_altitude}m altitude\")\n",
    "                print(f\"      Challenging: {worst_altitude}m altitude\")\n",
    "        \n",
    "        # Synthetic test insights\n",
    "        if 'synthetic_tests' in self.validation_results:\n",
    "            synthetic = self.validation_results['synthetic_tests']\n",
    "            if synthetic['identity_success_rate'] > 0.8:\n",
    "                print(f\"   ✅ Strong identity detection (perfect match capability)\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Identity detection needs improvement\")\n",
    "        \n",
    "        print(f\"\\n🔄 VALIDATION SUMMARY:\")\n",
    "        \n",
    "        if 'ground_truth_validation' in self.validation_results:\n",
    "            gt_metrics = self.validation_results['ground_truth_validation']['metrics']\n",
    "            \n",
    "            print(f\"   🎯 SPATIAL LOCALIZATION:\")\n",
    "            print(f\"      • Can find location within 50px: {gt_metrics['within_50_pixels']:.0%} of the time\")\n",
    "            print(f\"      • Average error: {gt_metrics['mean_distance_pixel']:.0f} pixels\")\n",
    "            print(f\"      • Best prediction is correct: {gt_metrics['top1_accuracy']:.0%} of the time\")\n",
    "            \n",
    "            # Real-world interpretation\n",
    "            # Assuming 10km TIF is about 1000 pixels wide, each pixel ≈ 10 meters\n",
    "            tif_width = self.env.tif_image.shape[1]\n",
    "            approx_meters_per_pixel = 10000 / tif_width  # 10km / width\n",
    "            error_meters = gt_metrics['mean_distance_pixel'] * approx_meters_per_pixel\n",
    "            \n",
    "            print(f\"   🌍 REAL-WORLD INTERPRETATION:\")\n",
    "            print(f\"      • Average localization error: ~{error_meters:.0f} meters\")\n",
    "            print(f\"      • Grid cell size: ~{approx_meters_per_pixel * (tif_width / self.env.grid_size):.0f} meters\")\n",
    "        \n",
    "        print(f\"\\n📈 NEXT STEPS:\")\n",
    "        \n",
    "        if 'ground_truth_validation' in self.validation_results:\n",
    "            gt_metrics = self.validation_results['ground_truth_validation']['metrics']\n",
    "            \n",
    "            if gt_metrics['within_50_pixels'] < 0.5:\n",
    "                print(f\"   • Train for more episodes to improve spatial accuracy\")\n",
    "                print(f\"   • Consider smaller grid size for finer localization\")\n",
    "                print(f\"   • Review reward function balance\")\n",
    "            \n",
    "            if gt_metrics['top1_accuracy'] < 0.4:\n",
    "                print(f\"   • Improve confidence calibration\")\n",
    "                print(f\"   • Adjust ranking rewards\")\n",
    "            \n",
    "            if gt_metrics['mean_distance_pixel'] > 200:\n",
    "                print(f\"   • Consider curriculum learning (easy crops first)\")\n",
    "                print(f\"   • Increase training data diversity\")\n",
    "        \n",
    "        print(f\"   • Test on additional TIF files for generalization\")\n",
    "        print(f\"   • Collect expert human evaluations\")\n",
    "        print(f\"   • Consider ensemble methods for improved accuracy\")\n",
    "        \n",
    "        # Save comprehensive validation results\n",
    "        validation_file = \"comprehensive_validation_results.json\"\n",
    "        with open(validation_file, 'w') as f:\n",
    "            # Convert numpy types for JSON serialization\n",
    "            json_results = {}\n",
    "            for key, value in self.validation_results.items():\n",
    "                if key == 'ground_truth_validation' and 'detailed_results' in value:\n",
    "                    # Skip detailed results for JSON (too large), keep metrics\n",
    "                    json_results[key] = {'metrics': value['metrics']}\n",
    "                elif isinstance(value, dict):\n",
    "                    json_results[key] = {k: float(v) if isinstance(v, np.number) else v \n",
    "                                       for k, v in value.items()}\n",
    "                else:\n",
    "                    json_results[key] = value\n",
    "            \n",
    "            json.dump(json_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n💾 Comprehensive validation results saved to: {validation_file}\")\n",
    "        \n",
    "        return grade, gt_metrics if 'ground_truth_validation' in self.validation_results else None\n",
    "\n",
    "def validate_drone_agent(trainer):\n",
    "    \"\"\"\n",
    "    Main validation function\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔍 Starting comprehensive validation...\")\n",
    "    \n",
    "    validator = DroneLocalizationValidator(trainer, validation_crops=15)\n",
    "    results = validator.run_comprehensive_validation()\n",
    "    \n",
    "    return validator, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔍 DRONE LOCALIZATION VALIDATION FRAMEWORK\")\n",
    "    print(\"=\"*50)\n",
    "    print()\n",
    "    print(\"🎯 VALIDATION METHODS:\")\n",
    "    print(\"   🥇 Ground truth validation (using center_pixel coordinates)\")\n",
    "    print(\"   • Visual validation (human-interpretable)\")\n",
    "    print(\"   • Quantitative metrics (success rates, correlations)\")\n",
    "    print(\"   • Synthetic test cases (known ground truth)\")\n",
    "    print(\"   • Cross-validation (by altitude/crop type)\")\n",
    "    print(\"   • Clustering analysis (spatial coherence)\")\n",
    "    print(\"   • Transfer testing (generalization)\")\n",
    "    print()\n",
    "    print(\"📊 GROUND TRUTH METRICS:\")\n",
    "    print(\"   • Spatial accuracy (distance to true location)\")\n",
    "    print(\"   • Top-1/Top-3 localization success\")\n",
    "    print(\"   • Real-world error in meters\")\n",
    "    print(\"   • Ranking quality analysis\")\n",
    "    print()\n",
    "    print(\"📊 TO RUN:\")\n",
    "    print(\"   # After training your agent:\")\n",
    "    print(\"   validator, results = validate_drone_agent(trainer)\")\n",
    "    print()\n",
    "    print(\"🏆 Provides definitive spatial accuracy assessment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m validator, results = validate_drone_agent(\u001b[43mtrainer\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "validator, results = validate_drone_agent(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
