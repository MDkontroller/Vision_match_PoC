{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NIKO9\\Desktop\\EW3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç OPTUNA HYPERPARAMETER OPTIMIZATION\n",
      "==================================================\n",
      "\n",
      "üéØ WHAT IT OPTIMIZES:\n",
      "   ‚Ä¢ Grid size (20-35)\n",
      "   ‚Ä¢ Network architecture (feature_dim, num_heads, dropout)\n",
      "   ‚Ä¢ Learning rate (1e-5 to 1e-2)\n",
      "   ‚Ä¢ PPO hyperparameters (eps_clip, k_epochs, entropy)\n",
      "   ‚Ä¢ Reward function weights\n",
      "\n",
      "‚è∞ OVERNIGHT MODE:\n",
      "   # Perfect for while you sleep!\n",
      "   study = quick_optimization_overnight()\n",
      "\n",
      "üî¨ MANUAL MODE:\n",
      "   # Custom trials and timeout\n",
      "   study = run_optimization(n_trials=100, timeout_hours=12)\n",
      "\n",
      "üöÄ DEPLOY BEST:\n",
      "   # Use best parameters found\n",
      "   trainer = create_optimized_trainer(study.best_params)\n",
      "   trainer.train_and_evaluate(num_episodes=1000)\n",
      "\n",
      "üò¥ Sweet dreams! Wake up to optimized hyperparameters!\n"
     ]
    }
   ],
   "source": [
    "# Optuna Hyperparameter Optimization for Drone Localization RL Agent\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Import your existing classes (assuming they're in the same file or imported)\n",
    "# from drone_localization_agent import DroneLocalizationEnvironment, PPODroneAgent, DroneLocalizationNetwork\n",
    "\n",
    "class OptimizedDroneLocalizationNetwork(torch.nn.Module):\n",
    "    \"\"\"Optimizable version of the drone localization network\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int, feature_dim: int, num_heads: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_locations = grid_size * grid_size\n",
    "        \n",
    "        # CNN backbone for image processing\n",
    "        self.tif_encoder = self._create_cnn_encoder(feature_dim, dropout_rate)\n",
    "        self.crop_encoder = self._create_cnn_encoder(feature_dim, dropout_rate)\n",
    "        \n",
    "        # Cross-attention between TIF and crop\n",
    "        self.cross_attention = torch.nn.MultiheadAttention(feature_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Spatial reasoning\n",
    "        self.spatial_reasoning = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim * 2, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(feature_dim, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.location_head = torch.nn.Linear(feature_dim, self.num_locations)\n",
    "        self.value_head = torch.nn.Linear(feature_dim, 1)\n",
    "        \n",
    "    def _create_cnn_encoder(self, feature_dim: int, dropout_rate: float):\n",
    "        \"\"\"Create optimizable CNN encoder\"\"\"\n",
    "        import torchvision.models as models\n",
    "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        encoder = torch.nn.Sequential(\n",
    "            *list(resnet.children())[:-2],\n",
    "            torch.nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512 * 64, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        return encoder\n",
    "    \n",
    "    def forward(self, tif_image: torch.Tensor, crop_image: torch.Tensor):\n",
    "        # Encode images\n",
    "        tif_features = self.tif_encoder(tif_image)\n",
    "        crop_features = self.crop_encoder(crop_image)\n",
    "        \n",
    "        # Cross-attention\n",
    "        tif_attended, _ = self.cross_attention(\n",
    "            crop_features.unsqueeze(1),\n",
    "            tif_features.unsqueeze(1),\n",
    "            tif_features.unsqueeze(1)\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([\n",
    "            tif_attended.squeeze(1), \n",
    "            crop_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Spatial reasoning\n",
    "        spatial_features = self.spatial_reasoning(combined_features)\n",
    "        \n",
    "        # Output predictions\n",
    "        location_logits = self.location_head(spatial_features)\n",
    "        location_probs = torch.nn.functional.softmax(location_logits, dim=1)\n",
    "        value = self.value_head(spatial_features)\n",
    "        \n",
    "        return location_probs, value, location_logits\n",
    "\n",
    "class OptimizedDroneLocalizationTrainer:\n",
    "    \"\"\"Optimizable trainer with configurable hyperparameters\"\"\"\n",
    "    \n",
    "    def __init__(self, trial_params: Dict[str, Any]):\n",
    "        self.params = trial_params\n",
    "        \n",
    "        # Initialize environment\n",
    "        self.env = DroneLocalizationEnvironment(\n",
    "            tif_image_path=None,\n",
    "            crops_metadata_path=None, \n",
    "            grid_size=trial_params['grid_size']\n",
    "        )\n",
    "        \n",
    "        # Initialize optimized agent\n",
    "        self.agent = self._create_optimized_agent(trial_params)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.episode_rewards = []\n",
    "        self.similarity_scores = []\n",
    "        self.convergence_episodes = []\n",
    "        \n",
    "    def _create_optimized_agent(self, params: Dict[str, Any]):\n",
    "        \"\"\"Create agent with optimized hyperparameters\"\"\"\n",
    "        \n",
    "        # Create optimized network\n",
    "        network = OptimizedDroneLocalizationNetwork(\n",
    "            grid_size=params['grid_size'],\n",
    "            feature_dim=params['feature_dim'],\n",
    "            num_heads=params['num_heads'],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Create agent-like object\n",
    "        class OptimizedAgent:\n",
    "            def __init__(self, network, optimizer, params):\n",
    "                self.network = network\n",
    "                self.optimizer = optimizer\n",
    "                self.grid_size = params['grid_size']\n",
    "                self.device = network.location_head.weight.device\n",
    "                \n",
    "                # PPO hyperparameters\n",
    "                self.gamma = params['gamma']\n",
    "                self.eps_clip = params['eps_clip']\n",
    "                self.k_epochs = params['k_epochs']\n",
    "                self.entropy_coef = params['entropy_coef']\n",
    "                self.value_coef = params['value_coef']\n",
    "                \n",
    "                self.memory = []\n",
    "            \n",
    "            def select_top3_actions(self, tif_image, crop_image):\n",
    "                # Preprocess images\n",
    "                tif_tensor = self._preprocess_image(tif_image).to(self.device)\n",
    "                crop_tensor = self._preprocess_image(crop_image).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    location_probs, value, logits = self.network(tif_tensor, crop_tensor)\n",
    "                    \n",
    "                    # Get top 3 predictions\n",
    "                    top_probs, top_indices = torch.topk(location_probs.squeeze(), k=3)\n",
    "                    \n",
    "                    # Convert to grid coordinates\n",
    "                    locations = []\n",
    "                    probabilities = []\n",
    "                    \n",
    "                    for i in range(3):\n",
    "                        idx = top_indices[i].item()\n",
    "                        prob = top_probs[i].item()\n",
    "                        \n",
    "                        grid_y = idx // self.grid_size\n",
    "                        grid_x = idx % self.grid_size\n",
    "                        \n",
    "                        locations.append((grid_x, grid_y))\n",
    "                        probabilities.append(prob)\n",
    "                \n",
    "                return locations, probabilities, logits.squeeze()\n",
    "            \n",
    "            def _preprocess_image(self, image):\n",
    "                import cv2\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                tensor = torch.from_numpy(resized).float() / 255.0\n",
    "                tensor = tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "                return tensor\n",
    "            \n",
    "            def store_experience(self, state, action_logits, reward, value):\n",
    "                self.memory.append({\n",
    "                    'state': state,\n",
    "                    'action_logits': action_logits.detach().cpu(),\n",
    "                    'reward': reward,\n",
    "                    'value': value.detach().cpu()\n",
    "                })\n",
    "            \n",
    "            def update_policy(self):\n",
    "                if len(self.memory) < 16:  # Minimum batch size\n",
    "                    return\n",
    "                \n",
    "                # Simplified PPO update for optimization\n",
    "                returns = []\n",
    "                advantages = []\n",
    "                gae = 0\n",
    "                \n",
    "                for i in reversed(range(len(self.memory))):\n",
    "                    if i == len(self.memory) - 1:\n",
    "                        next_value = 0\n",
    "                    else:\n",
    "                        next_value = self.memory[i + 1]['value']\n",
    "                    \n",
    "                    reward = self.memory[i]['reward']\n",
    "                    value = self.memory[i]['value']\n",
    "                    \n",
    "                    returns.insert(0, reward + self.gamma * next_value)\n",
    "                    \n",
    "                    delta = reward + self.gamma * next_value - value\n",
    "                    gae = delta + self.gamma * 0.95 * gae\n",
    "                    advantages.insert(0, gae)\n",
    "                \n",
    "                # Convert to tensors\n",
    "                returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "                advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Normalize advantages\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # PPO update\n",
    "                for _ in range(self.k_epochs):\n",
    "                    total_loss = 0\n",
    "                    \n",
    "                    for i, experience in enumerate(self.memory):\n",
    "                        state = experience['state']\n",
    "                        old_logits = experience['action_logits'].to(self.device)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        tif_tensor = self._preprocess_image(state['tif_image']).to(self.device)\n",
    "                        crop_tensor = self._preprocess_image(state['crop_image']).to(self.device)\n",
    "                        \n",
    "                        location_probs, value, new_logits = self.network(tif_tensor, crop_tensor)\n",
    "                        \n",
    "                        # Policy loss\n",
    "                        old_probs = torch.nn.functional.softmax(old_logits, dim=0)\n",
    "                        new_probs = torch.nn.functional.softmax(new_logits.squeeze(), dim=0)\n",
    "                        \n",
    "                        ratio = (new_probs + 1e-8) / (old_probs + 1e-8)\n",
    "                        \n",
    "                        surr1 = ratio * advantages[i]\n",
    "                        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages[i]\n",
    "                        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                        \n",
    "                        # Value loss\n",
    "                        value_loss = torch.nn.functional.mse_loss(value.squeeze(), returns[i])\n",
    "                        \n",
    "                        # Entropy loss\n",
    "                        entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-8))\n",
    "                        \n",
    "                        total_loss += (policy_loss + \n",
    "                                     self.value_coef * value_loss - \n",
    "                                     self.entropy_coef * entropy)\n",
    "                    \n",
    "                    # Update\n",
    "                    self.optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                # Clear memory\n",
    "                self.memory = []\n",
    "        \n",
    "        return OptimizedAgent(network, optimizer, params)\n",
    "    \n",
    "    def train_and_evaluate(self, num_episodes: int = 300, update_frequency: int = 16) -> float:\n",
    "        \"\"\"Train and return objective score for Optuna\"\"\"\n",
    "        \n",
    "        print(f\"   Training with params: {self.params}\")\n",
    "        \n",
    "        episode_rewards = []\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            \n",
    "            # Agent prediction\n",
    "            locations, probabilities, action_logits = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "            \n",
    "            # Get value estimate\n",
    "            with torch.no_grad():\n",
    "                tif_tensor = self.agent._preprocess_image(tif_image).to(self.agent.device)\n",
    "                crop_tensor = self.agent._preprocess_image(crop_image).to(self.agent.device)\n",
    "                _, value, _ = self.agent.network(tif_tensor, crop_tensor)\n",
    "            \n",
    "            # Store experience\n",
    "            self.agent.store_experience(state, action_logits, reward, value)\n",
    "            \n",
    "            # Update policy\n",
    "            if (episode + 1) % update_frequency == 0:\n",
    "                self.agent.update_policy()\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_rewards.append(reward)\n",
    "            similarity_scores.append(max(similarities))\n",
    "            \n",
    "            # Early stopping if not improving\n",
    "            if episode > 100 and episode % 50 == 0:\n",
    "                recent_avg = np.mean(episode_rewards[-50:])\n",
    "                older_avg = np.mean(episode_rewards[-100:-50])\n",
    "                \n",
    "                if recent_avg <= older_avg * 1.01:  # Less than 1% improvement\n",
    "                    print(f\"   Early stopping at episode {episode} (no improvement)\")\n",
    "                    break\n",
    "        \n",
    "        # Calculate final objective\n",
    "        final_episodes = min(100, len(episode_rewards))\n",
    "        final_reward = np.mean(episode_rewards[-final_episodes:])\n",
    "        final_similarity = np.mean(similarity_scores[-final_episodes:])\n",
    "        max_confidence = max([max(self.agent.select_top3_actions(\n",
    "            *self.env.reset()[:2])[1]) for _ in range(10)])\n",
    "        \n",
    "        # Combined objective (reward + similarity + confidence)\n",
    "        objective = 0.4 * final_reward + 0.4 * final_similarity + 0.2 * max_confidence\n",
    "        \n",
    "        print(f\"   Final objective: {objective:.4f} (reward: {final_reward:.3f}, \"\n",
    "              f\"similarity: {final_similarity:.3f}, confidence: {max_confidence:.3f})\")\n",
    "        \n",
    "        return objective\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function\"\"\"\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    params = {\n",
    "        # Grid and architecture\n",
    "        'grid_size': trial.suggest_int('grid_size', 20, 35),\n",
    "        'feature_dim': trial.suggest_categorical('feature_dim', [256, 384, 512, 768]),\n",
    "        'num_heads': trial.suggest_categorical('num_heads', [4, 6, 8, 12]),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        \n",
    "        # Learning parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.95, 0.999),\n",
    "        \n",
    "        # PPO parameters\n",
    "        'eps_clip': trial.suggest_float('eps_clip', 0.1, 0.3),\n",
    "        'k_epochs': trial.suggest_int('k_epochs', 2, 8),\n",
    "        'entropy_coef': trial.suggest_float('entropy_coef', 0.001, 0.1, log=True),\n",
    "        'value_coef': trial.suggest_float('value_coef', 0.1, 1.0),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create trainer with sampled parameters\n",
    "        trainer = OptimizedDroneLocalizationTrainer(params)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        objective_score = trainer.train_and_evaluate(num_episodes=300)\n",
    "        \n",
    "        return objective_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Trial failed with error: {e}\")\n",
    "        return 0.0  # Return poor score for failed trials\n",
    "\n",
    "def run_optimization(n_trials: int = 50, timeout_hours: int = 4):\n",
    "    \"\"\"Run Optuna optimization\"\"\"\n",
    "    \n",
    "    # Setup logging\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler())\n",
    "    optuna.logging.set_verbosity(logging.INFO)\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=50)\n",
    "    )\n",
    "    \n",
    "    print(f\"üîç Starting Optuna Optimization\")\n",
    "    print(f\"   Trials: {n_trials}\")\n",
    "    print(f\"   Timeout: {timeout_hours} hours\")\n",
    "    print(f\"   Start time: {datetime.now()}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    start_time = time.time()\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout_hours * 3600,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\nüéâ Optimization Complete!\")\n",
    "    print(f\"   Duration: {(end_time - start_time) / 3600:.2f} hours\")\n",
    "    print(f\"   Trials completed: {len(study.trials)}\")\n",
    "    print(f\"   Best value: {study.best_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'n_trials': len(study.trials),\n",
    "        'duration_hours': (end_time - start_time) / 3600,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    results_file = f\"optuna_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"   üíæ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Plot optimization history (if possible)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Optimization history\n",
    "        optuna.visualization.matplotlib.plot_optimization_history(study, ax=ax1)\n",
    "        ax1.set_title('Optimization History')\n",
    "        \n",
    "        # Parameter importances\n",
    "        optuna.visualization.matplotlib.plot_param_importances(study, ax=ax2)\n",
    "        ax2.set_title('Parameter Importances')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"optuna_plots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\", dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   üìä Install matplotlib and plotly for visualization\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "def create_optimized_trainer(best_params: Dict[str, Any]):\n",
    "    \"\"\"Create trainer with best parameters found by Optuna\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Creating optimized trainer with best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return OptimizedDroneLocalizationTrainer(best_params)\n",
    "\n",
    "def quick_optimization_overnight():\n",
    "    \"\"\"Quick function to run overnight optimization\"\"\"\n",
    "    \n",
    "    print(\"üåô Starting Overnight Optimization...\")\n",
    "    print(\"   This will run for 8 hours or 50 trials (whichever comes first)\")\n",
    "    print(\"   Go to sleep! Results will be ready in the morning üò¥\")\n",
    "    \n",
    "    study = run_optimization(n_trials=50, timeout_hours=8)\n",
    "    \n",
    "    print(f\"\\n‚òÄÔ∏è Good morning! Optimization complete.\")\n",
    "    print(f\"   Use these parameters for your next training:\")\n",
    "    print(f\"   {study.best_params}\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\"*50)\n",
    "    print()\n",
    "    print(\"üéØ WHAT IT OPTIMIZES:\")\n",
    "    print(\"   ‚Ä¢ Grid size (20-35)\")\n",
    "    print(\"   ‚Ä¢ Network architecture (feature_dim, num_heads, dropout)\")\n",
    "    print(\"   ‚Ä¢ Learning rate (1e-5 to 1e-2)\")\n",
    "    print(\"   ‚Ä¢ PPO hyperparameters (eps_clip, k_epochs, entropy)\")\n",
    "    print(\"   ‚Ä¢ Reward function weights\")\n",
    "    print()\n",
    "    print(\"‚è∞ OVERNIGHT MODE:\")\n",
    "    print(\"   # Perfect for while you sleep!\")\n",
    "    print(\"   study = quick_optimization_overnight()\")\n",
    "    print()\n",
    "    print(\"üî¨ MANUAL MODE:\")\n",
    "    print(\"   # Custom trials and timeout\")\n",
    "    print(\"   study = run_optimization(n_trials=100, timeout_hours=12)\")\n",
    "    print()\n",
    "    print(\"üöÄ DEPLOY BEST:\")\n",
    "    print(\"   # Use best parameters found\")\n",
    "    print(\"   trainer = create_optimized_trainer(study.best_params)\")\n",
    "    print(\"   trainer.train_and_evaluate(num_episodes=1000)\")\n",
    "    print()\n",
    "    print(\"üò¥ Sweet dreams! Wake up to optimized hyperparameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memory safe optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç OPTUNA HYPERPARAMETER OPTIMIZATION\n",
      "==================================================\n",
      "\n",
      "üéØ WHAT IT OPTIMIZES:\n",
      "   ‚Ä¢ Grid size (20-35)\n",
      "   ‚Ä¢ Network architecture (feature_dim, num_heads, dropout)\n",
      "   ‚Ä¢ Learning rate (1e-5 to 1e-2)\n",
      "   ‚Ä¢ PPO hyperparameters (eps_clip, k_epochs, entropy)\n",
      "   ‚Ä¢ Reward function weights\n",
      "\n",
      "‚è∞ OVERNIGHT MODE:\n",
      "   # Perfect for while you sleep!\n",
      "   study = quick_optimization_overnight()\n",
      "\n",
      "üî¨ MANUAL MODE:\n",
      "   # Custom trials and timeout\n",
      "   study = run_optimization(n_trials=100, timeout_hours=12)\n",
      "\n",
      "üöÄ DEPLOY BEST:\n",
      "   # Use best parameters found\n",
      "   trainer = create_optimized_trainer(study.best_params)\n",
      "   trainer.train_and_evaluate(num_episodes=1000)\n",
      "\n",
      "üò¥ Sweet dreams! Wake up to optimized hyperparameters!\n"
     ]
    }
   ],
   "source": [
    "# Optuna Hyperparameter Optimization for Drone Localization RL Agent\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Import your existing classes (assuming they're in the same file or imported)\n",
    "# from drone_localization_agent import DroneLocalizationEnvironment, PPODroneAgent, DroneLocalizationNetwork\n",
    "\n",
    "class OptimizedDroneLocalizationNetwork(torch.nn.Module):\n",
    "    \"\"\"Optimizable version of the drone localization network\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size: int, feature_dim: int, num_heads: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_locations = grid_size * grid_size\n",
    "        \n",
    "        # CNN backbone for image processing\n",
    "        self.tif_encoder = self._create_cnn_encoder(feature_dim, dropout_rate)\n",
    "        self.crop_encoder = self._create_cnn_encoder(feature_dim, dropout_rate)\n",
    "        \n",
    "        # Cross-attention between TIF and crop\n",
    "        self.cross_attention = torch.nn.MultiheadAttention(feature_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Spatial reasoning\n",
    "        self.spatial_reasoning = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim * 2, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(feature_dim, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.location_head = torch.nn.Linear(feature_dim, self.num_locations)\n",
    "        self.value_head = torch.nn.Linear(feature_dim, 1)\n",
    "        \n",
    "    def _create_cnn_encoder(self, feature_dim: int, dropout_rate: float):\n",
    "        \"\"\"Create optimizable CNN encoder\"\"\"\n",
    "        import torchvision.models as models\n",
    "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        encoder = torch.nn.Sequential(\n",
    "            *list(resnet.children())[:-2],\n",
    "            torch.nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512 * 64, feature_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        return encoder\n",
    "    \n",
    "    def forward(self, tif_image: torch.Tensor, crop_image: torch.Tensor):\n",
    "        # Encode images\n",
    "        tif_features = self.tif_encoder(tif_image)\n",
    "        crop_features = self.crop_encoder(crop_image)\n",
    "        \n",
    "        # Cross-attention\n",
    "        tif_attended, _ = self.cross_attention(\n",
    "            crop_features.unsqueeze(1),\n",
    "            tif_features.unsqueeze(1),\n",
    "            tif_features.unsqueeze(1)\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([\n",
    "            tif_attended.squeeze(1), \n",
    "            crop_features\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Spatial reasoning\n",
    "        spatial_features = self.spatial_reasoning(combined_features)\n",
    "        \n",
    "        # Output predictions\n",
    "        location_logits = self.location_head(spatial_features)\n",
    "        location_probs = torch.nn.functional.softmax(location_logits, dim=1)\n",
    "        value = self.value_head(spatial_features)\n",
    "        \n",
    "        return location_probs, value, location_logits\n",
    "\n",
    "class OptimizedDroneLocalizationTrainer:\n",
    "    \"\"\"Optimizable trainer with configurable hyperparameters\"\"\"\n",
    "    \n",
    "    def __init__(self, trial_params: Dict[str, Any]):\n",
    "        self.params = trial_params\n",
    "        \n",
    "        # Initialize environment\n",
    "        self.env = DroneLocalizationEnvironment(\n",
    "            tif_image_path=None,\n",
    "            crops_metadata_path=None, \n",
    "            grid_size=trial_params['grid_size']\n",
    "        )\n",
    "        \n",
    "        # Initialize optimized agent\n",
    "        self.agent = self._create_optimized_agent(trial_params)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.episode_rewards = []\n",
    "        self.similarity_scores = []\n",
    "        self.convergence_episodes = []\n",
    "        \n",
    "    def _create_optimized_agent(self, params: Dict[str, Any]):\n",
    "        \"\"\"Create agent with optimized hyperparameters\"\"\"\n",
    "        \n",
    "        # Create optimized network\n",
    "        network = OptimizedDroneLocalizationNetwork(\n",
    "            grid_size=params['grid_size'],\n",
    "            feature_dim=params['feature_dim'],\n",
    "            num_heads=params['num_heads'],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Create agent-like object\n",
    "        class OptimizedAgent:\n",
    "            def __init__(self, network, optimizer, params):\n",
    "                self.network = network\n",
    "                self.optimizer = optimizer\n",
    "                self.grid_size = params['grid_size']\n",
    "                self.device = network.location_head.weight.device\n",
    "                \n",
    "                # PPO hyperparameters\n",
    "                self.gamma = params['gamma']\n",
    "                self.eps_clip = params['eps_clip']\n",
    "                self.k_epochs = params['k_epochs']\n",
    "                self.entropy_coef = params['entropy_coef']\n",
    "                self.value_coef = params['value_coef']\n",
    "                \n",
    "                self.memory = []\n",
    "            \n",
    "            def select_top3_actions(self, tif_image, crop_image):\n",
    "                # Preprocess images\n",
    "                tif_tensor = self._preprocess_image(tif_image).to(self.device)\n",
    "                crop_tensor = self._preprocess_image(crop_image).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    location_probs, value, logits = self.network(tif_tensor, crop_tensor)\n",
    "                    \n",
    "                    # Get top 3 predictions\n",
    "                    top_probs, top_indices = torch.topk(location_probs.squeeze(), k=3)\n",
    "                    \n",
    "                    # Convert to grid coordinates\n",
    "                    locations = []\n",
    "                    probabilities = []\n",
    "                    \n",
    "                    for i in range(3):\n",
    "                        idx = top_indices[i].item()\n",
    "                        prob = top_probs[i].item()\n",
    "                        \n",
    "                        grid_y = idx // self.grid_size\n",
    "                        grid_x = idx % self.grid_size\n",
    "                        \n",
    "                        locations.append((grid_x, grid_y))\n",
    "                        probabilities.append(prob)\n",
    "                \n",
    "                return locations, probabilities, logits.squeeze()\n",
    "            \n",
    "            def _preprocess_image(self, image):\n",
    "                import cv2\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                tensor = torch.from_numpy(resized).float() / 255.0\n",
    "                tensor = tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "                return tensor\n",
    "            \n",
    "            def store_experience(self, state, action_logits, reward, value):\n",
    "                self.memory.append({\n",
    "                    'state': state,\n",
    "                    'action_logits': action_logits.detach().cpu(),\n",
    "                    'reward': reward,\n",
    "                    'value': value.detach().cpu()\n",
    "                })\n",
    "            \n",
    "            def update_policy(self):\n",
    "                if len(self.memory) < 16:  # Minimum batch size\n",
    "                    return\n",
    "                \n",
    "                # Simplified PPO update for optimization\n",
    "                returns = []\n",
    "                advantages = []\n",
    "                gae = 0\n",
    "                \n",
    "                for i in reversed(range(len(self.memory))):\n",
    "                    if i == len(self.memory) - 1:\n",
    "                        next_value = 0\n",
    "                    else:\n",
    "                        next_value = self.memory[i + 1]['value']\n",
    "                    \n",
    "                    reward = self.memory[i]['reward']\n",
    "                    value = self.memory[i]['value']\n",
    "                    \n",
    "                    returns.insert(0, reward + self.gamma * next_value)\n",
    "                    \n",
    "                    delta = reward + self.gamma * next_value - value\n",
    "                    gae = delta + self.gamma * 0.95 * gae\n",
    "                    advantages.insert(0, gae)\n",
    "                \n",
    "                # Convert to tensors\n",
    "                returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "                advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Normalize advantages\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                # PPO update\n",
    "                for _ in range(self.k_epochs):\n",
    "                    total_loss = 0\n",
    "                    \n",
    "                    for i, experience in enumerate(self.memory):\n",
    "                        state = experience['state']\n",
    "                        old_logits = experience['action_logits'].to(self.device)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        tif_tensor = self._preprocess_image(state['tif_image']).to(self.device)\n",
    "                        crop_tensor = self._preprocess_image(state['crop_image']).to(self.device)\n",
    "                        \n",
    "                        location_probs, value, new_logits = self.network(tif_tensor, crop_tensor)\n",
    "                        \n",
    "                        # Policy loss\n",
    "                        old_probs = torch.nn.functional.softmax(old_logits, dim=0)\n",
    "                        new_probs = torch.nn.functional.softmax(new_logits.squeeze(), dim=0)\n",
    "                        \n",
    "                        ratio = (new_probs + 1e-8) / (old_probs + 1e-8)\n",
    "                        \n",
    "                        surr1 = ratio * advantages[i]\n",
    "                        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages[i]\n",
    "                        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                        \n",
    "                        # Value loss\n",
    "                        value_loss = torch.nn.functional.mse_loss(value.squeeze(), returns[i])\n",
    "                        \n",
    "                        # Entropy loss\n",
    "                        entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-8))\n",
    "                        \n",
    "                        total_loss += (policy_loss + \n",
    "                                     self.value_coef * value_loss - \n",
    "                                     self.entropy_coef * entropy)\n",
    "                    \n",
    "                    # Update\n",
    "                    self.optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                # Clear memory\n",
    "                self.memory = []\n",
    "        \n",
    "        return OptimizedAgent(network, optimizer, params)\n",
    "    \n",
    "    def train_and_evaluate(self, num_episodes: int = 300, update_frequency: int = 16) -> float:\n",
    "        \"\"\"Train and return objective score for Optuna\"\"\"\n",
    "        \n",
    "        print(f\"   Training with params: {self.params}\")\n",
    "        \n",
    "        episode_rewards = []\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Reset environment\n",
    "            tif_image, crop_image, state = self.env.reset()\n",
    "            \n",
    "            # Agent prediction\n",
    "            locations, probabilities, action_logits = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "            \n",
    "            # Get value estimate\n",
    "            with torch.no_grad():\n",
    "                tif_tensor = self.agent._preprocess_image(tif_image).to(self.agent.device)\n",
    "                crop_tensor = self.agent._preprocess_image(crop_image).to(self.agent.device)\n",
    "                _, value, _ = self.agent.network(tif_tensor, crop_tensor)\n",
    "            \n",
    "            # Store experience\n",
    "            self.agent.store_experience(state, action_logits, reward, value)\n",
    "            \n",
    "            # Update policy\n",
    "            if (episode + 1) % update_frequency == 0:\n",
    "                self.agent.update_policy()\n",
    "            \n",
    "            # Track metrics\n",
    "            episode_rewards.append(reward)\n",
    "            similarity_scores.append(max(similarities))\n",
    "            \n",
    "            # Early stopping if not improving\n",
    "            if episode > 100 and episode % 50 == 0:\n",
    "                recent_avg = np.mean(episode_rewards[-50:])\n",
    "                older_avg = np.mean(episode_rewards[-100:-50])\n",
    "                \n",
    "                if recent_avg <= older_avg * 1.01:  # Less than 1% improvement\n",
    "                    print(f\"   Early stopping at episode {episode} (no improvement)\")\n",
    "                    break\n",
    "        \n",
    "        # Calculate final objective\n",
    "        final_episodes = min(100, len(episode_rewards))\n",
    "        final_reward = np.mean(episode_rewards[-final_episodes:])\n",
    "        final_similarity = np.mean(similarity_scores[-final_episodes:])\n",
    "        max_confidence = max([max(self.agent.select_top3_actions(\n",
    "            *self.env.reset()[:2])[1]) for _ in range(10)])\n",
    "        \n",
    "        # Combined objective (reward + similarity + confidence)\n",
    "        objective = 0.4 * final_reward + 0.4 * final_similarity + 0.2 * max_confidence\n",
    "        \n",
    "        print(f\"   Final objective: {objective:.4f} (reward: {final_reward:.3f}, \"\n",
    "              f\"similarity: {final_similarity:.3f}, confidence: {max_confidence:.3f})\")\n",
    "        \n",
    "        return objective\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function\"\"\"\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    params = {\n",
    "        # Grid and architecture\n",
    "        'grid_size': trial.suggest_int('grid_size', 20, 35),\n",
    "        'feature_dim': trial.suggest_categorical('feature_dim', [256, 384, 512, 768]),\n",
    "        'num_heads': trial.suggest_categorical('num_heads', [4, 6, 8, 12]),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        \n",
    "        # Learning parameters\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.95, 0.999),\n",
    "        \n",
    "        # PPO parameters\n",
    "        'eps_clip': trial.suggest_float('eps_clip', 0.1, 0.3),\n",
    "        'k_epochs': trial.suggest_int('k_epochs', 2, 8),\n",
    "        'entropy_coef': trial.suggest_float('entropy_coef', 0.001, 0.1, log=True),\n",
    "        'value_coef': trial.suggest_float('value_coef', 0.1, 1.0),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Create trainer with sampled parameters\n",
    "        trainer = OptimizedDroneLocalizationTrainer(params)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        objective_score = trainer.train_and_evaluate(num_episodes=300)\n",
    "        \n",
    "        return objective_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Trial failed with error: {e}\")\n",
    "        return 0.0  # Return poor score for failed trials\n",
    "\n",
    "def run_optimization(n_trials: int = 50, timeout_hours: int = 8):\n",
    "    \"\"\"Run Optuna optimization\"\"\"\n",
    "    \n",
    "    # Setup logging\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler())\n",
    "    optuna.logging.set_verbosity(logging.INFO)\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=50)\n",
    "    )\n",
    "    \n",
    "    print(f\"üîç Starting Optuna Optimization\")\n",
    "    print(f\"   Trials: {n_trials}\")\n",
    "    print(f\"   Timeout: {timeout_hours} hours\")\n",
    "    print(f\"   Start time: {datetime.now()}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    start_time = time.time()\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout_hours * 3600,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\nüéâ Optimization Complete!\")\n",
    "    print(f\"   Duration: {(end_time - start_time) / 3600:.2f} hours\")\n",
    "    print(f\"   Trials completed: {len(study.trials)}\")\n",
    "    print(f\"   Best value: {study.best_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'n_trials': len(study.trials),\n",
    "        'duration_hours': (end_time - start_time) / 3600,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    results_file = f\"optuna_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"   üíæ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Plot optimization history (if possible)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Optimization history\n",
    "        optuna.visualization.matplotlib.plot_optimization_history(study, ax=ax1)\n",
    "        ax1.set_title('Optimization History')\n",
    "        \n",
    "        # Parameter importances\n",
    "        optuna.visualization.matplotlib.plot_param_importances(study, ax=ax2)\n",
    "        ax2.set_title('Parameter Importances')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"optuna_plots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\", dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   üìä Install matplotlib and plotly for visualization\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "def create_optimized_trainer(best_params: Dict[str, Any]):\n",
    "    \"\"\"Create trainer with best parameters found by Optuna\"\"\"\n",
    "    \n",
    "    print(f\"üöÄ Creating optimized trainer with best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return OptimizedDroneLocalizationTrainer(best_params)\n",
    "\n",
    "# Optuna Hyperparameter Optimization for Drone Localization RL Agent\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "import traceback\n",
    "\n",
    "# Memory management utilities\n",
    "class MemoryManager:\n",
    "    \"\"\"Manages memory and prevents OOM during optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, max_gpu_memory_gb: float = 10.0, max_cpu_memory_gb: float = 16.0):\n",
    "        self.max_gpu_memory = max_gpu_memory_gb * 1024 * 1024 * 1024  # Convert to bytes\n",
    "        self.max_cpu_memory = max_cpu_memory_gb * 1024 * 1024 * 1024\n",
    "        \n",
    "    def check_memory_limits(self) -> bool:\n",
    "        \"\"\"Check if we're approaching memory limits\"\"\"\n",
    "        try:\n",
    "            # Check CPU memory\n",
    "            cpu_memory = psutil.virtual_memory()\n",
    "            cpu_used = cpu_memory.used\n",
    "            \n",
    "            if cpu_used > self.max_cpu_memory:\n",
    "                print(f\"   ‚ö†Ô∏è CPU memory limit exceeded: {cpu_used / 1024**3:.1f}GB\")\n",
    "                return False\n",
    "            \n",
    "            # Check GPU memory if available\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.memory_allocated()\n",
    "                if gpu_memory > self.max_gpu_memory:\n",
    "                    print(f\"   ‚ö†Ô∏è GPU memory limit exceeded: {gpu_memory / 1024**3:.1f}GB\")\n",
    "                    return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Memory check failed: {e}\")\n",
    "            return True  # Continue if we can't check\n",
    "    \n",
    "    def cleanup_memory(self):\n",
    "        \"\"\"Aggressive memory cleanup\"\"\"\n",
    "        try:\n",
    "            # Clear Python garbage\n",
    "            gc.collect()\n",
    "            \n",
    "            # Clear CUDA cache if available\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Force garbage collection again\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Memory cleanup failed: {e}\")\n",
    "    \n",
    "    def get_memory_info(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        info = {}\n",
    "        \n",
    "        try:\n",
    "            # CPU memory\n",
    "            cpu_memory = psutil.virtual_memory()\n",
    "            info['cpu_used_gb'] = cpu_memory.used / 1024**3\n",
    "            info['cpu_percent'] = cpu_memory.percent\n",
    "            \n",
    "            # GPU memory\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.memory_allocated()\n",
    "                gpu_reserved = torch.cuda.memory_reserved()\n",
    "                info['gpu_used_gb'] = gpu_memory / 1024**3\n",
    "                info['gpu_reserved_gb'] = gpu_reserved / 1024**3\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Memory info failed: {e}\")\n",
    "        \n",
    "        return info\n",
    "\n",
    "class SafeOptimizedDroneLocalizationTrainer:\n",
    "    \"\"\"Memory-safe trainer with proper cleanup\"\"\"\n",
    "    \n",
    "    def __init__(self, trial_params: Dict[str, Any]):\n",
    "        self.params = trial_params\n",
    "        self.memory_manager = MemoryManager()\n",
    "        self.env = None\n",
    "        self.agent = None\n",
    "        \n",
    "        try:\n",
    "            # Check memory before initialization\n",
    "            if not self.memory_manager.check_memory_limits():\n",
    "                raise MemoryError(\"Memory limits exceeded before initialization\")\n",
    "            \n",
    "            # Initialize with memory monitoring\n",
    "            self._safe_initialize()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.cleanup()\n",
    "            raise e\n",
    "    \n",
    "    def _safe_initialize(self):\n",
    "        \"\"\"Initialize with memory safety\"\"\"\n",
    "        \n",
    "        # Limit batch sizes based on grid size to prevent OOM\n",
    "        grid_size = self.params['grid_size']\n",
    "        if grid_size > 30:\n",
    "            max_memory_per_episode = 100  # MB\n",
    "        else:\n",
    "            max_memory_per_episode = 200  # MB\n",
    "        \n",
    "        # Initialize environment\n",
    "        self.env = DroneLocalizationEnvironment(\n",
    "            tif_image_path=None,\n",
    "            crops_metadata_path=None, \n",
    "            grid_size=grid_size\n",
    "        )\n",
    "        \n",
    "        # Check memory after env creation\n",
    "        if not self.memory_manager.check_memory_limits():\n",
    "            raise MemoryError(\"Memory limits exceeded after environment creation\")\n",
    "        \n",
    "        # Initialize agent with memory limits\n",
    "        self.agent = self._create_memory_safe_agent(self.params)\n",
    "        \n",
    "        # Final memory check\n",
    "        if not self.memory_manager.check_memory_limits():\n",
    "            raise MemoryError(\"Memory limits exceeded after agent creation\")\n",
    "    \n",
    "    def _create_memory_safe_agent(self, params: Dict[str, Any]):\n",
    "        \"\"\"Create agent with memory limitations\"\"\"\n",
    "        \n",
    "        # Reduce feature dimensions if memory is tight\n",
    "        available_memory = psutil.virtual_memory().available / 1024**3\n",
    "        if available_memory < 8:  # Less than 8GB available\n",
    "            params['feature_dim'] = min(params['feature_dim'], 256)\n",
    "            params['num_heads'] = min(params['num_heads'], 4)\n",
    "        \n",
    "        # Create network with memory monitoring\n",
    "        try:\n",
    "            network = OptimizedDroneLocalizationNetwork(\n",
    "                grid_size=params['grid_size'],\n",
    "                feature_dim=params['feature_dim'],\n",
    "                num_heads=params['num_heads'],\n",
    "                dropout_rate=params['dropout_rate']\n",
    "            )\n",
    "            \n",
    "            # Move to GPU only if we have enough memory\n",
    "            device = 'cpu'\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    network = network.to('cuda')\n",
    "                    device = 'cuda'\n",
    "                    # Test allocation\n",
    "                    test_tensor = torch.randn(1, 3, 224, 224).to('cuda')\n",
    "                    _ = network.tif_encoder(test_tensor)\n",
    "                    del test_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e).lower():\n",
    "                        print(f\"   ‚ö†Ô∏è GPU OOM, falling back to CPU\")\n",
    "                        network = network.to('cpu')\n",
    "                        device = 'cpu'\n",
    "                        torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        raise e\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.memory_manager.cleanup_memory()\n",
    "            raise e\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Create memory-safe agent\n",
    "        class MemorySafeAgent:\n",
    "            def __init__(self, network, optimizer, params, memory_manager):\n",
    "                self.network = network\n",
    "                self.optimizer = optimizer\n",
    "                self.grid_size = params['grid_size']\n",
    "                self.device = device\n",
    "                self.memory_manager = memory_manager\n",
    "                \n",
    "                # PPO hyperparameters\n",
    "                self.gamma = params['gamma']\n",
    "                self.eps_clip = params['eps_clip']\n",
    "                self.k_epochs = params['k_epochs']\n",
    "                self.entropy_coef = params['entropy_coef']\n",
    "                self.value_coef = params['value_coef']\n",
    "                \n",
    "                # Memory-limited storage\n",
    "                max_memory_size = min(32, 64 // (params['grid_size'] // 20))\n",
    "                self.memory = []\n",
    "                self.max_memory_size = max_memory_size\n",
    "            \n",
    "            def select_top3_actions(self, tif_image, crop_image):\n",
    "                try:\n",
    "                    # Check memory before processing\n",
    "                    if not self.memory_manager.check_memory_limits():\n",
    "                        raise MemoryError(\"Memory limit exceeded during action selection\")\n",
    "                    \n",
    "                    # Preprocess images\n",
    "                    tif_tensor = self._preprocess_image(tif_image).to(self.device)\n",
    "                    crop_tensor = self._preprocess_image(crop_image).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        location_probs, value, logits = self.network(tif_tensor, crop_tensor)\n",
    "                        \n",
    "                        # Get top 3 predictions\n",
    "                        top_probs, top_indices = torch.topk(location_probs.squeeze(), k=3)\n",
    "                        \n",
    "                        # Convert to grid coordinates\n",
    "                        locations = []\n",
    "                        probabilities = []\n",
    "                        \n",
    "                        for i in range(3):\n",
    "                            idx = top_indices[i].item()\n",
    "                            prob = top_probs[i].item()\n",
    "                            \n",
    "                            grid_y = idx // self.grid_size\n",
    "                            grid_x = idx % self.grid_size\n",
    "                            \n",
    "                            locations.append((grid_x, grid_y))\n",
    "                            probabilities.append(prob)\n",
    "                    \n",
    "                    # Clean up tensors\n",
    "                    del tif_tensor, crop_tensor\n",
    "                    if self.device == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    return locations, probabilities, logits.squeeze()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.memory_manager.cleanup_memory()\n",
    "                    raise e\n",
    "            \n",
    "            def _preprocess_image(self, image):\n",
    "                import cv2\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                tensor = torch.from_numpy(resized).float() / 255.0\n",
    "                tensor = tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "                return tensor\n",
    "            \n",
    "            def store_experience(self, state, action_logits, reward, value):\n",
    "                # Limit memory size to prevent OOM\n",
    "                if len(self.memory) >= self.max_memory_size:\n",
    "                    self.memory.pop(0)  # Remove oldest experience\n",
    "                \n",
    "                self.memory.append({\n",
    "                    'state': state,\n",
    "                    'action_logits': action_logits.detach().cpu(),\n",
    "                    'reward': reward,\n",
    "                    'value': value.detach().cpu()\n",
    "                })\n",
    "            \n",
    "            def update_policy(self):\n",
    "                try:\n",
    "                    if len(self.memory) < 8:  # Reduced minimum batch size\n",
    "                        return\n",
    "                    \n",
    "                    # Check memory before update\n",
    "                    if not self.memory_manager.check_memory_limits():\n",
    "                        print(\"   ‚ö†Ô∏è Skipping policy update due to memory limits\")\n",
    "                        return\n",
    "                    \n",
    "                    # Simplified PPO update with memory management\n",
    "                    returns = []\n",
    "                    advantages = []\n",
    "                    gae = 0\n",
    "                    \n",
    "                    for i in reversed(range(len(self.memory))):\n",
    "                        if i == len(self.memory) - 1:\n",
    "                            next_value = 0\n",
    "                        else:\n",
    "                            next_value = self.memory[i + 1]['value']\n",
    "                        \n",
    "                        reward = self.memory[i]['reward']\n",
    "                        value = self.memory[i]['value']\n",
    "                        \n",
    "                        returns.insert(0, reward + self.gamma * next_value)\n",
    "                        \n",
    "                        delta = reward + self.gamma * next_value - value\n",
    "                        gae = delta + self.gamma * 0.95 * gae\n",
    "                        advantages.insert(0, gae)\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "                    advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    # Normalize advantages\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                    \n",
    "                    # Reduced PPO epochs for memory safety\n",
    "                    safe_k_epochs = min(self.k_epochs, 3)\n",
    "                    \n",
    "                    # PPO update with memory monitoring\n",
    "                    for epoch in range(safe_k_epochs):\n",
    "                        total_loss = 0\n",
    "                        \n",
    "                        # Process in smaller batches if needed\n",
    "                        batch_size = min(len(self.memory), 8)\n",
    "                        \n",
    "                        for i in range(0, len(self.memory), batch_size):\n",
    "                            batch_end = min(i + batch_size, len(self.memory))\n",
    "                            batch_loss = 0\n",
    "                            \n",
    "                            for j in range(i, batch_end):\n",
    "                                experience = self.memory[j]\n",
    "                                state = experience['state']\n",
    "                                old_logits = experience['action_logits'].to(self.device)\n",
    "                                \n",
    "                                # Forward pass\n",
    "                                tif_tensor = self._preprocess_image(state['tif_image']).to(self.device)\n",
    "                                crop_tensor = self._preprocess_image(state['crop_image']).to(self.device)\n",
    "                                \n",
    "                                location_probs, value, new_logits = self.network(tif_tensor, crop_tensor)\n",
    "                                \n",
    "                                # Policy loss\n",
    "                                old_probs = torch.nn.functional.softmax(old_logits, dim=0)\n",
    "                                new_probs = torch.nn.functional.softmax(new_logits.squeeze(), dim=0)\n",
    "                                \n",
    "                                ratio = (new_probs + 1e-8) / (old_probs + 1e-8)\n",
    "                                \n",
    "                                surr1 = ratio * advantages[j]\n",
    "                                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages[j]\n",
    "                                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                                \n",
    "                                # Value loss\n",
    "                                value_loss = torch.nn.functional.mse_loss(value.squeeze(), returns[j])\n",
    "                                \n",
    "                                # Entropy loss\n",
    "                                entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-8))\n",
    "                                \n",
    "                                batch_loss += (policy_loss + \n",
    "                                             self.value_coef * value_loss - \n",
    "                                             self.entropy_coef * entropy)\n",
    "                                \n",
    "                                # Clean up tensors\n",
    "                                del tif_tensor, crop_tensor\n",
    "                            \n",
    "                            # Update with batch\n",
    "                            if batch_loss != 0:\n",
    "                                self.optimizer.zero_grad()\n",
    "                                batch_loss.backward()\n",
    "                                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                                self.optimizer.step()\n",
    "                            \n",
    "                            # Memory cleanup between batches\n",
    "                            if self.device == 'cuda':\n",
    "                                torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Check memory between epochs\n",
    "                        if not self.memory_manager.check_memory_limits():\n",
    "                            print(f\"   ‚ö†Ô∏è Stopping PPO early at epoch {epoch} due to memory\")\n",
    "                            break\n",
    "                    \n",
    "                    # Clear memory after update\n",
    "                    self.memory = []\n",
    "                    self.memory_manager.cleanup_memory()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Policy update failed: {e}\")\n",
    "                    self.memory_manager.cleanup_memory()\n",
    "                    self.memory = []  # Clear memory on failure\n",
    "        \n",
    "        return MemorySafeAgent(network, optimizer, params, self.memory_manager)\n",
    "    \n",
    "    def train_and_evaluate(self, num_episodes: int = 200, update_frequency: int = 16) -> float:\n",
    "        \"\"\"Memory-safe training with monitoring\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"   üîß Training with params: {self.params}\")\n",
    "            mem_info = self.memory_manager.get_memory_info()\n",
    "            print(f\"   üíæ Memory: CPU {mem_info.get('cpu_used_gb', 0):.1f}GB, \"\n",
    "                  f\"GPU {mem_info.get('gpu_used_gb', 0):.1f}GB\")\n",
    "            \n",
    "            episode_rewards = []\n",
    "            similarity_scores = []\n",
    "            memory_failures = 0\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                try:\n",
    "                    # Check memory before each episode\n",
    "                    if not self.memory_manager.check_memory_limits():\n",
    "                        print(f\"   ‚ö†Ô∏è Memory limit reached at episode {episode}\")\n",
    "                        break\n",
    "                    \n",
    "                    # Reset environment\n",
    "                    tif_image, crop_image, state = self.env.reset()\n",
    "                    \n",
    "                    # Agent prediction\n",
    "                    locations, probabilities, action_logits = self.agent.select_top3_actions(tif_image, crop_image)\n",
    "                    \n",
    "                    # Calculate reward\n",
    "                    reward, similarities = self.env.calculate_reward(locations, probabilities)\n",
    "                    \n",
    "                    # Get value estimate\n",
    "                    with torch.no_grad():\n",
    "                        tif_tensor = self.agent._preprocess_image(tif_image).to(self.agent.device)\n",
    "                        crop_tensor = self.agent._preprocess_image(crop_image).to(self.agent.device)\n",
    "                        _, value, _ = self.agent.network(tif_tensor, crop_tensor)\n",
    "                        \n",
    "                        # Clean up\n",
    "                        del tif_tensor, crop_tensor\n",
    "                        if self.agent.device == 'cuda':\n",
    "                            torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Store experience\n",
    "                    self.agent.store_experience(state, action_logits, reward, value)\n",
    "                    \n",
    "                    # Update policy\n",
    "                    if (episode + 1) % update_frequency == 0:\n",
    "                        self.agent.update_policy()\n",
    "                    \n",
    "                    # Track metrics\n",
    "                    episode_rewards.append(reward)\n",
    "                    similarity_scores.append(max(similarities))\n",
    "                    \n",
    "                    # Periodic memory cleanup\n",
    "                    if episode % 50 == 0:\n",
    "                        self.memory_manager.cleanup_memory()\n",
    "                    \n",
    "                    # Early stopping if not improving\n",
    "                    if episode > 100 and episode % 50 == 0:\n",
    "                        recent_avg = np.mean(episode_rewards[-25:])\n",
    "                        older_avg = np.mean(episode_rewards[-50:-25])\n",
    "                        \n",
    "                        if recent_avg <= older_avg * 1.01:\n",
    "                            print(f\"   üìà Early stopping at episode {episode} (no improvement)\")\n",
    "                            break\n",
    "                    \n",
    "                except MemoryError as e:\n",
    "                    memory_failures += 1\n",
    "                    print(f\"   ‚ö†Ô∏è Memory error at episode {episode}: {e}\")\n",
    "                    self.memory_manager.cleanup_memory()\n",
    "                    \n",
    "                    if memory_failures > 3:\n",
    "                        print(f\"   ‚ùå Too many memory failures, stopping trial\")\n",
    "                        break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Episode {episode} failed: {e}\")\n",
    "                    self.memory_manager.cleanup_memory()\n",
    "                    continue\n",
    "            \n",
    "            # Calculate final objective\n",
    "            if len(episode_rewards) < 10:\n",
    "                print(f\"   ‚ùå Trial failed - insufficient episodes completed\")\n",
    "                return 0.0\n",
    "            \n",
    "            final_episodes = min(50, len(episode_rewards))\n",
    "            final_reward = np.mean(episode_rewards[-final_episodes:])\n",
    "            final_similarity = np.mean(similarity_scores[-final_episodes:])\n",
    "            \n",
    "            # Test confidence\n",
    "            try:\n",
    "                confidence_scores = []\n",
    "                for _ in range(5):\n",
    "                    tif_img, crop_img, _ = self.env.reset()\n",
    "                    _, probs, _ = self.agent.select_top3_actions(tif_img, crop_img)\n",
    "                    confidence_scores.extend(probs)\n",
    "                max_confidence = max(confidence_scores) if confidence_scores else 0.0\n",
    "            except:\n",
    "                max_confidence = 0.0\n",
    "            \n",
    "            # Combined objective\n",
    "            objective = 0.4 * final_reward + 0.4 * final_similarity + 0.2 * max_confidence\n",
    "            \n",
    "            print(f\"   ‚úÖ Final objective: {objective:.4f} (reward: {final_reward:.3f}, \"\n",
    "                  f\"similarity: {final_similarity:.3f}, confidence: {max_confidence:.3f})\")\n",
    "            \n",
    "            return objective\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Training failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Always cleanup\n",
    "            self.cleanup()\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        try:\n",
    "            if hasattr(self, 'agent') and self.agent:\n",
    "                if hasattr(self.agent, 'memory'):\n",
    "                    self.agent.memory = []\n",
    "                del self.agent\n",
    "            \n",
    "            if hasattr(self, 'env') and self.env:\n",
    "                del self.env\n",
    "            \n",
    "            self.memory_manager.cleanup_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Cleanup failed: {e}\")\n",
    "\n",
    "def safe_objective(trial):\n",
    "    \"\"\"Memory-safe Optuna objective function with proper error handling\"\"\"\n",
    "    \n",
    "    trainer = None\n",
    "    try:\n",
    "        # Sample hyperparameters with memory considerations\n",
    "        params = {\n",
    "            # Grid and architecture - conservative limits\n",
    "            'grid_size': trial.suggest_int('grid_size', 20, 35),\n",
    "            'feature_dim': trial.suggest_categorical('feature_dim', [256, 384, 512]),  # Removed 768\n",
    "            'num_heads': trial.suggest_categorical('num_heads', [4, 6, 8]),  # Removed 12\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.4),\n",
    "            \n",
    "            # Learning parameters\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 0.95, 0.999),\n",
    "            \n",
    "            # PPO parameters\n",
    "            'eps_clip': trial.suggest_float('eps_clip', 0.1, 0.3),\n",
    "            'k_epochs': trial.suggest_int('k_epochs', 2, 6),  # Reduced max\n",
    "            'entropy_coef': trial.suggest_float('entropy_coef', 0.001, 0.1, log=True),\n",
    "            'value_coef': trial.suggest_float('value_coef', 0.1, 1.0),\n",
    "        }\n",
    "        \n",
    "        # Check available memory before starting\n",
    "        available_memory = psutil.virtual_memory().available / 1024**3\n",
    "        if available_memory < 4:  # Less than 4GB available\n",
    "            print(f\"   ‚ö†Ô∏è Low memory ({available_memory:.1f}GB), pruning trial\")\n",
    "            raise optuna.TrialPruned(\"Insufficient memory\")\n",
    "        \n",
    "        # Create trainer with memory safety\n",
    "        trainer = SafeOptimizedDroneLocalizationTrainer(params)\n",
    "        \n",
    "        # Train and evaluate with shorter episodes for safety\n",
    "        objective_score = trainer.train_and_evaluate(num_episodes=200)\n",
    "        \n",
    "        # Prune trials with poor early performance\n",
    "        if objective_score < 0.1:\n",
    "            raise optuna.TrialPruned(\"Poor performance\")\n",
    "        \n",
    "        return objective_score\n",
    "        \n",
    "    except MemoryError as e:\n",
    "        print(f\"   üí• Memory error: {e}\")\n",
    "        raise optuna.TrialPruned(\"Out of memory\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"   üí• CUDA OOM: {e}\")\n",
    "            raise optuna.TrialPruned(\"CUDA out of memory\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Runtime error: {e}\")\n",
    "            return 0.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Trial failed: {e}\")\n",
    "        print(f\"   Stack trace: {traceback.format_exc()}\")\n",
    "        return 0.0\n",
    "        \n",
    "    finally:\n",
    "        # Always cleanup\n",
    "        if trainer:\n",
    "            trainer.cleanup()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def run_safe_optimization(n_trials: int = 30, timeout_hours: int = 8):\n",
    "    \"\"\"Run memory-safe Optuna optimization\"\"\"\n",
    "    \n",
    "    # Setup logging\n",
    "    optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler())\n",
    "    optuna.logging.set_verbosity(logging.INFO)\n",
    "    \n",
    "    # Create study with aggressive pruning\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=3,  # Start pruning early\n",
    "            n_warmup_steps=25,   # Prune quickly\n",
    "            interval_steps=10    # Check frequently\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"üîç Starting Memory-Safe Optuna Optimization\")\n",
    "    print(f\"   Trials: {n_trials}\")\n",
    "    print(f\"   Timeout: {timeout_hours} hours\")\n",
    "    print(f\"   Memory limits: GPU 10GB, CPU 16GB\")\n",
    "    print(f\"   Start time: {datetime.now()}\")\n",
    "    \n",
    "    # Check system memory\n",
    "    total_memory = psutil.virtual_memory().total / 1024**3\n",
    "    available_memory = psutil.virtual_memory().available / 1024**3\n",
    "    print(f\"   System memory: {available_memory:.1f}GB available / {total_memory:.1f}GB total\")\n",
    "    \n",
    "    if available_memory < 8:\n",
    "        print(f\"   ‚ö†Ô∏è Warning: Low available memory, consider closing other applications\")\n",
    "    \n",
    "    # Run optimization\n",
    "    start_time = time.time()\n",
    "    study.optimize(\n",
    "        safe_objective, \n",
    "        n_trials=n_trials,\n",
    "        timeout=timeout_hours * 3600,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True  # Force garbage collection after each trial\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\nüéâ Optimization Complete!\")\n",
    "    print(f\"   Duration: {(end_time - start_time) / 3600:.2f} hours\")\n",
    "    print(f\"   Trials completed: {len(study.trials)}\")\n",
    "    print(f\"   Trials pruned: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "    print(f\"   Best value: {study.best_value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'n_trials': len(study.trials),\n",
    "        'n_pruned': len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]),\n",
    "        'duration_hours': (end_time - start_time) / 3600,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    results_file = f\"safe_optuna_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"   üíæ Results saved to: {results_file}\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "def safe_overnight_optimization():\n",
    "    \"\"\"Memory-safe overnight optimization\"\"\"\n",
    "    \n",
    "    print(\"üåô Starting Safe Overnight Optimization...\")\n",
    "    print(\"   Memory-safe with aggressive pruning and cleanup\")\n",
    "    print(\"   Will automatically handle OOM and other errors\")\n",
    "    print(\"   Sweet dreams! üò¥\")\n",
    "    \n",
    "    study = run_safe_optimization(n_trials=30, timeout_hours=8)\n",
    "    \n",
    "    print(f\"\\n‚òÄÔ∏è Good morning! Safe optimization complete.\")\n",
    "    print(f\"   Best parameters found: {study.best_params}\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\"*50)\n",
    "    print()\n",
    "    print(\"üéØ WHAT IT OPTIMIZES:\")\n",
    "    print(\"   ‚Ä¢ Grid size (20-35)\")\n",
    "    print(\"   ‚Ä¢ Network architecture (feature_dim, num_heads, dropout)\")\n",
    "    print(\"   ‚Ä¢ Learning rate (1e-5 to 1e-2)\")\n",
    "    print(\"   ‚Ä¢ PPO hyperparameters (eps_clip, k_epochs, entropy)\")\n",
    "    print(\"   ‚Ä¢ Reward function weights\")\n",
    "    print()\n",
    "    print(\"‚è∞ OVERNIGHT MODE:\")\n",
    "    print(\"   # Perfect for while you sleep!\")\n",
    "    print(\"   study = quick_optimization_overnight()\")\n",
    "    print()\n",
    "    print(\"üî¨ MANUAL MODE:\")\n",
    "    print(\"   # Custom trials and timeout\")\n",
    "    print(\"   study = run_optimization(n_trials=100, timeout_hours=12)\")\n",
    "    print()\n",
    "    print(\"üöÄ DEPLOY BEST:\")\n",
    "    print(\"   # Use best parameters found\")\n",
    "    print(\"   trainer = create_optimized_trainer(study.best_params)\")\n",
    "    print(\"   trainer.train_and_evaluate(num_episodes=1000)\")\n",
    "    print()\n",
    "    print(\"üò¥ Sweet dreams! Wake up to optimized hyperparameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
